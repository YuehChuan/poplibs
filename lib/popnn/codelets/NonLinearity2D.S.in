#ifdef __IPU__

// Assembly implementation of popnn::NonLinearity2D vertex template instantiations.

// Restrictions
//
//  * Vertex state aligned to at least 4 bytes.

#include "colossus/tilearch.h"
#include "colossus/tileimplconsts.h"

// Symbols
#define HALF_SYMBOL \
  __runCodelet_popnn__NonLinearity2D___half_popnn__NonLinearityType__@NL_TYPE_UPPER@
#define FLOAT_SYMBOL \
  __runCodelet_popnn__NonLinearity2D___float_popnn__NonLinearityType__@NL_TYPE_UPPER@

// Constants
#define BASE_AND_N0_VOFFSET 0
#define DELTAN_PTR_VOFFSET 2

#define DELTAN_BASE_PTR_BITS 20
#define DELTAN_BASE_PTR_MASK ((1 << DELTAN_BASE_PTR_BITS) - 1)
#define DELTAN_OFFSET_BITS 18
#define DELTAN_OFFSET_MASK ((1 << DELTAN_OFFSET_BITS) - 1)

// Worker register aliases
#define MASK m0
#define MEMORY_BASE m1
#define BASE_PTR m2
#define N0 m3
#define DELTAN_PTR m4
#define DATA_PTR m5
#define N1 m6
#define N1_64BIT m7
#define MSCRATCH m10

#define ACTS_0 a0
#define ACTS_1 a1
#define ACTS_PAIR a0:1
#define RESULTS_0 a4
#define RESULTS_1 a5
#define RESULTS_PAIR a4:5
#define ASCRATCH a6
#define ASCRATCH_PAIR a6:7

// Macros
#define v1 // Scalar
#define VECTOR_INSTRUCTION(type, vector_width, op) type ## vector_width ## op

#define CALC_SIGMOID(type, vector_width, dst, src) \
  VECTOR_INSTRUCTION(type, vector_width, sigm) dst, src
#define CALC_RELU(type, vector_width, dst, src) \
  VECTOR_INSTRUCTION(type, vector_width, max) dst, src, $azero
#define CALC_TANH(type, vector_width, dst, src) \
  VECTOR_INSTRUCTION(type, vector_width, tanh) dst, src

#define CALC(type, vector_width, dst, src) \
  CALC_@NL_TYPE_UPPER@(type, vector_width, dst, src)

.section .text.HALF_SYMBOL
.globl HALF_SYMBOL
.type HALF_SYMBOL, @function

.align 8
    nop
HALF_SYMBOL:
    ld32 $MSCRATCH, $mvertex_base, $mzero, BASE_AND_N0_VOFFSET
    ldz16 $DELTAN_PTR, $mvertex_base, $mzero, DELTAN_PTR_VOFFSET

    // Unpack base pointer and n0
    setzi $MASK, DELTAN_BASE_PTR_MASK
    and $BASE_PTR, $MSCRATCH, $MASK
    shr $N0, $MSCRATCH, DELTAN_BASE_PTR_BITS

    // DeltaN table pointer is a ScaledPtr32, gives offset in
    // 32-bit units from TMEM_REGION0_BASE_ADDR
    setzi $MEMORY_BASE, TMEM_REGION0_BASE_ADDR
    shl $DELTAN_PTR, $DELTAN_PTR, 2

    setzi $MASK, DELTAN_OFFSET_MASK

    // Top-level loop through each DeltaN
    add $N0, $N0, -1
.Lhalf_n0_loop:
    ld32step $MSCRATCH, $MEMORY_BASE, $DELTAN_PTR+=, 1
    and $DATA_PTR, $MSCRATCH, $MASK
    shr $N1, $MSCRATCH, DELTAN_OFFSET_BITS
    // Actually offset DATA_PTR so that below alignment checks
    // take BASE_PTR alignment into account
    add $DATA_PTR, $BASE_PTR, $DATA_PTR

    and $MSCRATCH, $DATA_PTR, 0x3
    brz $MSCRATCH, .Lhalf_32_bit_aligned

    // Handle the first 16-bit element. We'll always have
    // at least 1 element here.
    andc $DATA_PTR, $DATA_PTR, 0x3
    ldb16 $ACTS_0, $DATA_PTR, $mzero, 1
    {
      ldb16 $ASCRATCH, $DATA_PTR, $mzero, 0
      CALC(f16, v2, $RESULTS_0, $ACTS_0)
    }
    roll16 $RESULTS_0, $ASCRATCH, $RESULTS_0
    st32step $RESULTS_0, $mzero, $DATA_PTR+=, 1
    add $N1, $N1, -1
    brz $N1, .Lhalf_n0_loop_cond

.Lhalf_32_bit_aligned:
    and $MSCRATCH, $DATA_PTR, 0x7
    brz $MSCRATCH, .Lhalf_64_bit_aligned

    // Special case for a single 16-bit element at 32-bit
    // aligned address.
    cmpult $MSCRATCH, $N1, 2
    brnz $MSCRATCH, .Lhalf_16_bit_remainder

    ld32 $ACTS_0, $DATA_PTR, $mzero, 0
    CALC(f16, v2, $RESULTS_0, $ACTS_0)
    st32step $RESULTS_0, $mzero, $DATA_PTR+=, 1
    add $N1, $N1, -2

.Lhalf_64_bit_aligned:
    shr $N1_64BIT, $N1, 2

    brz $N1_64BIT, .Lhalf_32_bit_remainder
    add $N1_64BIT, $N1_64BIT, -1
    ld64 $ACTS_PAIR, $DATA_PTR, $mzero, 0
    {
      rpt $N1_64BIT, (2f - 1f) / 8 - 1
      CALC(f16, v2, $RESULTS_0, $ACTS_0)
    }
1:
    {
      ld64 $ACTS_PAIR, $DATA_PTR, $mzero, 1
      CALC(f16, v2, $RESULTS_1, $ACTS_1)
    }
    {
      st64step $RESULTS_PAIR, $mzero, $DATA_PTR+=, 1
      CALC(f16, v2, $RESULTS_0, $ACTS_0)
    }
2:
    CALC(f16, v2, $RESULTS_1, $ACTS_1)
    st64step $RESULTS_PAIR, $mzero, $DATA_PTR+=, 1

.Lhalf_32_bit_remainder:
    and $MSCRATCH, $N1, 0x2
    brz $MSCRATCH, .Lhalf_16_bit_remainder

    ld32 $ACTS_0, $DATA_PTR, $mzero, 0
    CALC(f16, v2, $RESULTS_0, $ACTS_0)
    st32step $RESULTS_0, $mzero, $DATA_PTR+=, 1

.Lhalf_16_bit_remainder:
    and $MSCRATCH, $N1, 0x1
    brz $MSCRATCH, .Lhalf_n0_loop_cond

    ldb16 $ACTS_0, $DATA_PTR, $mzero, 0
    {
      ldb16 $ASCRATCH, $DATA_PTR, $mzero, 1
      CALC(f16, v2, $RESULTS_0, $ACTS_0)
    }
    roll16 $RESULTS_0, $RESULTS_0, $ASCRATCH
    st32step $RESULTS_0, $mzero, $DATA_PTR+=, 1

.Lhalf_n0_loop_cond:
    brnzdec $N0, .Lhalf_n0_loop
    exitz $mzero

.size HALF_SYMBOL, .-HALF_SYMBOL

.section .text.FLOAT_SYMBOL
.globl FLOAT_SYMBOL
.type FLOAT_SYMBOL, @function

.align 8
    nop
FLOAT_SYMBOL:
    ld32 $MSCRATCH, $mvertex_base, $mzero, BASE_AND_N0_VOFFSET
    ldz16 $DELTAN_PTR, $mvertex_base, $mzero, DELTAN_PTR_VOFFSET

    // Unpack base pointer and n0
    setzi $MASK, DELTAN_BASE_PTR_MASK
    and $BASE_PTR, $MSCRATCH, $MASK
    shr $N0, $MSCRATCH, DELTAN_BASE_PTR_BITS

    // DeltaN table pointer is a ScaledPtr32, gives offset in
    // 32-bit units from TMEM_REGION0_BASE_ADDR
    setzi $MEMORY_BASE, TMEM_REGION0_BASE_ADDR
    shl $DELTAN_PTR, $DELTAN_PTR, 2

    setzi $MASK, DELTAN_OFFSET_MASK

    // Top-level loop through each DeltaN
    add $N0, $N0, -1
.Lfloat_n0_loop:
    ld32step $MSCRATCH, $MEMORY_BASE, $DELTAN_PTR+=, 1
    and $DATA_PTR, $MSCRATCH, $MASK
    shr $N1, $MSCRATCH, DELTAN_OFFSET_BITS
    // Actually offset DATA_PTR so that below alignment checks
    // take BASE_PTR alignment into account
    add $DATA_PTR, $BASE_PTR, $DATA_PTR

    // DATA_PTR and N1 give us the regions to actually loop
.Lfloat_32_bit_aligned:
    and $MSCRATCH, $DATA_PTR, 0x7
    brz $MSCRATCH, .Lfloat_64_bit_aligned

    // Handle the first 32-bit element. We'll always have
    // at least 1 element here.
    ld32 $ACTS_0, $DATA_PTR, $mzero, 0
    CALC(f32, v1, $RESULTS_0, $ACTS_0)
    st32step $RESULTS_0, $mzero, $DATA_PTR+=, 1
    add $N1, $N1, -1

.Lfloat_64_bit_aligned:
    shr $N1_64BIT, $N1, 1

    brz $N1_64BIT, .Lfloat_32_bit_remainder
    add $N1_64BIT, $N1_64BIT, -1
    ld64 $ACTS_PAIR, $DATA_PTR, $mzero, 0
    {
      rpt $N1_64BIT, (2f - 1f) / 8 - 1
      CALC(f32, v1, $RESULTS_0, $ACTS_0)
    }
1:
    {
      ld64 $ACTS_PAIR, $DATA_PTR, $mzero, 1
      CALC(f32, v1, $RESULTS_1, $ACTS_1)
    }
    {
      st64step $RESULTS_PAIR, $mzero, $DATA_PTR+=, 1
      CALC(f32, v1, $RESULTS_0, $ACTS_0)
    }
2:
    CALC(f32, v1, $RESULTS_1, $ACTS_1)
    st64step $RESULTS_PAIR, $mzero, $DATA_PTR+=, 1

.Lfloat_32_bit_remainder:
    and $MSCRATCH, $N1, 0x1
    brz $MSCRATCH, .Lfloat_n0_loop_cond

    ld32 $ACTS_0, $DATA_PTR, $mzero, 0
    CALC(f32, v1, $RESULTS_0, $ACTS_0)
    st32step $RESULTS_0, $mzero, $DATA_PTR+=, 1

.Lfloat_n0_loop_cond:
    brnzdec $N0, .Lfloat_n0_loop
    exitz $mzero

.size FLOAT_SYMBOL, .-FLOAT_SYMBOL

#endif // __IPU__
