// Copyright (c) 2018 Graphcore Ltd, All rights reserved.
//
// Contains functions to calculate partials for convolution. Partials and Output
// are used interchangebly in this file. Each worker may process part of a
// contiguous field. This is  done by setting up a partition which contains an
// input offset in the input channel group, an output offset in the output field
// and the number of field elements to process.
//

#ifdef __IPU__

#include "poplar/AvailableVTypes.h"
#include "poplibs_support/TileConstants.hpp"
#define ZAACC_BITMASK (CSR_W_FP_CLR__ZAACC__MASK << CSR_W_FP_CLR__ZAACC__SHIFT)

// Set this to 1 if the state is retained between calls to workers
#define WORKER_REG_STATE_RETAINED  0

// =============================================================================

// The type of weight loading supported
#define LD128                     0  // use ld128putcs
#define LD64                      1  // use ld64putcs

// =============================================================================

#define CODELET_NAME_128 __runCodelet_poplin__ConvPartial1x1Out___half_float_true_true
#define CODELET_NAME_64 __runCodelet_poplin__ConvPartial1x1Out___half_float_true_false

// =============================================================================

// Number of outputs generated by AMP
#define AMP_OUTPUTS               8
#define LOG2_AMP_OUTPUTS          3
#define SIZEOF_FLOAT              4
#define SIZEOF_HALF               2

// The number of input channel groups is fixed to 16
#define NUM_INCHAN_GROUPS         16

// =============================================================================

//// Supervisor vertex state
////
#if defined(VECTOR_AVAIL_SCALED_PTR32)
// Pointer to input channel group deltas
#define SUP_INCHAN_VECTORS        0  // scaled32, ushort
// Pointer to weights vectors
#define SUP_WEIGHTS_VECTORS       2  // scaled32, ushort
// Pointer to partials
#define SUP_OUTCHAN_VECTORS       4  // scaled32, ushort
// Pointer to worker partition table
#define SUP_PARTITION             6  // scaled32, ushort
// Number of convolution groups
#define SUP_NUM_CONV_GROUPS_M1    8   // short
// Number of contiguous output channel group fields. Value is 1 less.
#define SUP_NUM_OUTCHAN_GROUPS_M1 10  // short
// Number of contiguous input channel group fields
#define SUP_NUM_INCHAN_GROUPS     12  // short
// input and output strides
#define SUP_INPUT_STRIDE          14  // short
// number of output channels per group
#define SUP_OUTCHANS_PER_GROUP    16  // short
// Processed out stride: the value depends on whether output is flipped or
// not. This vertex does not actually support striding but this field is used
// to directly create a stride for the flipped output
#define SUP_OUTSTRIDE             18 // short

#else
// Pointer to input channel group deltas
#define SUP_INCHAN_VECTORS        0  // one_ptr, unsigned
// Pointer to weights vectors
#define SUP_WEIGHTS_VECTORS       4  // one_ptr, unsigned
// Pointer to partials
#define SUP_OUTCHAN_VECTORS       8  // one_ptr, unsigned
// Pointer to worker partition table
#define SUP_PARTITION             12  // one_ptr, unsigned
// Number of convolution groups
#define SUP_NUM_CONV_GROUPS_M1    16  // short
// Number of contiguous output channel group fields. Value is 1 less.
#define SUP_NUM_OUTCHAN_GROUPS_M1 18  // short
// Number of contiguous input channel group fields
#define SUP_NUM_INCHAN_GROUPS     20  // short
// input and output strides
#define SUP_INPUT_STRIDE          22  // short
// number of output channels per group
#define SUP_OUTCHANS_PER_GROUP    24  // short
// Processed out stride: the value depends on whether output is flipped or
// not. This vertex does not actually support striding but this field is used
// to directly create a stride for the flipped output
#define SUP_OUTSTRIDE             26 // short

#endif // #if defined(VECTOR_AVAIL_SCALED_PTR32)

// =============================================================================

//// Vertex state shared between workers (Worker vertex state is allocated
//// on supervisor stack and along with stack space used by supervisor must be
//// a multiple of 8 bytes)
////
// Pointer to input channel group field
#define WKR_INCHAN_PTR            0    // word
// Pointer to output/partial channel group field
#define WKR_OUTCHAN_PTR           4    // word
// Input stride
#define WKR_IN_OUT_STRIDES        8   // word
// Gap to be left in bytes after all partials are written for a field position
#define WKR_OUTCHANS_PER_GROUP    12   // word
#define WKR_PARTITION             16
#define WKR_VERTEX_SIZE           24   // bytes

// =============================================================================

.section ".text.convPartialFlattenedField", "ax"
.type convPartialFlattenedField, @function
.align 8
.worker

// worker code:
//
// first_in_group is set to a 1 for the first input channel group of every
// convolution, in which case no partial is loaded
//
// first_in_group = 1: (i.e. zeroing of partials)
//        num_field_pos = 0
//                      14
//        num_field_pos == 1
//                      27
//                   num_field_pos == 2
//                      29
//                   num_field_pos >= 3
//                      30  + (num_field_pos - 3) * 4
//
//
// first_in_group = 0: (i.e. no zeroing of partials)
//        num_field_pos = 0
//                      14
//        num_field_pos == 1
//                      29
//                   num_field_pos == 2
//                      31
//                   num_field_pos >= 3
//                      32  + (num_field_pos - 3) * 4
//
//
// The very first call of the worker requires 12 more cycles if 
//    WORKER_REG_STATE_RETAINED == 1
// Otherwise all calls of workers requires 12 cycles more.

// Total:
convPartialFlattenedFieldAligned:
nop
convPartialFlattenedField:

#define wkr_id                    m0
#define inchan_ptr                m0
#define outchan_ptr               m1
#define tripacked_addr            m0:1
#define partition_w               m2
#define num_outchans_per_groupx4  m2
#define in_off                    m3
#define out_off                   m4
#define num_elems                 m5
#define stride1                   m6
#define zero_outchan              m6
#define cmp_res                   m7
#define eq2flag                   m8
#define stride3                   m10
#define const_stride_m2           m11

get           $wkr_id, $WSR
and           $wkr_id, $wkr_id, CSR_W_WSR__CTXTID_M1__MASK

// each partition is 3 ushorts
mul           $wkr_id, $wkr_id, 6
ld32          $partition_w, $mvertex_base, WKR_PARTITION/4

ldz16step     $out_off, $wkr_id, $partition_w+=, 1
ldz16step     $num_elems, $wkr_id, $partition_w+=, 1
ldz16step     $in_off, $wkr_id, $partition_w+=, 1
ld32          $num_outchans_per_groupx4, $mvertex_base, WKR_OUTCHANS_PER_GROUP/4
mul           $out_off, $out_off, $num_outchans_per_groupx4
mul           $in_off, $in_off, NUM_INCHAN_GROUPS * SIZEOF_HALF
add           $num_elems, $num_elems, -3
setzi         $const_stride_m2, 0xFF800  // -2 * 1024

convPartialFlattenedFieldStateRetained:
ld32          $stride1, $mvertex_base, WKR_IN_OUT_STRIDES/4
{
  ld32          $inchan_ptr, $mvertex_base, WKR_INCHAN_PTR/4
  setzi         $a0, ZAACC_BITMASK
}
{
  ld32          $outchan_ptr, $mvertex_base, WKR_OUTCHAN_PTR/4
  uput          $FP_CLR, $a0
}

add           $inchan_ptr, $inchan_ptr, $in_off
add           $outchan_ptr, $outchan_ptr, $out_off
brpos         $zero_outchan, LNoPartialsToLoad

Amp_start:

// can do a ld128 as partials are always in interleaved memory and the
// second pointer increment doesn not affect the store pointer
ld128step     $a0:3, $mzero, $outchan_ptr+=, 1
{
  // Get compact representation of physical addresses
  tapack        $tripacked_addr, $inchan_ptr, $outchan_ptr, $outchan_ptr
  f16v4sisoamp  $a6:7, $azeros, $a0:1, TAMP_F16V4_E4_P0
}
{
  ld2x64pace    $azeros, $a4:5, $tripacked_addr+=, $mzero, 0b0011
  f16v4sisoamp  $a6:7, $azeros, $a2:3, TAMP_F16V4_E4_P1
}
brneg         $num_elems, NumElemsEq1AndEq2
{
  ld2x64pace    $azeros, $a2:3, $tripacked_addr+=, $stride1, 0b1011
  f16v4sisoamp  $a6:7, $azeros, $a4:5, TAMP_F16V4_E4_P2
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $mzero, 0b0000
  f16v4sisoamp  $a6:7, $azeros, $a2:3, TAMP_F16V4_E4_P3
}
{
  // restore write pointer to correct address after writing original value
  ld2xst64pace  $a0:3, $a4:5, $tripacked_addr+=, $const_stride_m2, 0b100000
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P0
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $mzero, 0b0000
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P1
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride1, 0b1001
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P2
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $mzero, 0b0000
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P3
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $mzero, 0b0000
  f16v4sisoamp  $a4:5, $a0:1, $a2:3, TAMP_F16V4_E4_P0
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $mzero, 0b0000
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P1
}

rpt $num_elems, (Loop_end_Amp-Loop_start_Amp)/8-1
Loop_start_Amp:
  // The reads in the last pass are effectively dummy to avoid code bloat
  {
    ld2xst64pace  $a0:3, $a4:5, $tripacked_addr+=, $stride1, 0b001001
    f16v4sisoamp  $a4:5, $a0:1, $a2:3, TAMP_F16V4_E4_P2
  }
  {
    ld2xst64pace  $a0:3, $a6:7, $tripacked_addr+=, $mzero, 0b000000
    f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P3
  }
  {
    ld2xst64pace  $a0:3, $a4:5, $tripacked_addr+=, $mzero, 0b000000
    f16v4sisoamp  $a4:5, $a0:1, $a2:3, TAMP_F16V4_E4_P0
  }
  {
    ld2xst64pace  $a0:3, $a6:7, $tripacked_addr+=, $stride1, 0b100000
    f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P1
  }
Loop_end_Amp:

{
  ld2xst64pace  $a0:3, $a4:5, $tripacked_addr+=, $stride1, 0b001101
  f16v4sisoamp  $a4:5, $a0:1, $a2:3, TAMP_F16V4_E4_P2
}
{
  ldst64pace    $a0:1, $a6:7, $tripacked_addr+=, $mzero, 0b0000
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P3
}
{
  ldst64pace    $a0:1, $a4:5, $tripacked_addr+=, $mzero, 0b0000
  f16v4sisoamp  $a4:5, $a0:1, $azeros, TAMP_F16V4_E4_P0
}
{
  ldst64pace    $a0:1, $a6:7, $tripacked_addr+=, $stride1, 0b1000
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P1
}
LNumElemsEq2:
{
  ldst64pace    $a0:1, $a4:5, $tripacked_addr+=, $mzero, 0b000
  f16v4sisoamp  $a4:5, $a0:1, $azeros, TAMP_F16V4_E4_P2
}
{
  st64pace      $a6:7, $tripacked_addr+=, $mzero, 0b00
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P3
}
{
  st64pace      $a4:5, $tripacked_addr+=, $mzero, 0b00
  f16v4sisoamp  $a4:5, $a0:1, $azeros, TAMP_F16V4_E4_P0
}
{
  st64pace      $a6:7, $tripacked_addr+=, $stride1, 0b10
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P1
}

LNumElemsEq1:

// This may need to change if partials for the next loop could be loaded
// with the store of old results
{
  st64pace      $a4:5,          $tripacked_addr+=, $mzero, 0b00
  f16v4sisoamp  $a4:5, $azeros, $azeros, TAMP_F16V4_E4_P2
}
{
  st64pace      $a6:7,          $tripacked_addr+=, $mzero, 0b00
  f16v4sisoamp  $a6:7, $azeros, $azeros, TAMP_F16V4_E4_P3
}
st64pace      $a4:5,          $tripacked_addr+=, $mzero, 0b00
st64pace      $a6:7,          $tripacked_addr+=, $mzero, 0b00

L_end_fn:
exitz         $m15

// Handles the case of number of elements <=2
// stride1 at any point contains strides for both input and output. These
// may be modified to avoid overreading partials
NumElemsEq1AndEq2:
// This code fragment is called if number of elements are 0, 1, or 2
add           $cmp_res, $num_elems, 1
cmpeq         $eq2flag, $cmp_res, $mzero
or            $stride3, $eq2flag, $const_stride_m2
brz           $cmp_res, SetStride1NumElemsEq1
add           $cmp_res, $cmp_res, 1
brneg         $cmp_res, L_end_fn
setzi         $stride1, 0

SetStride1NumElemsEq1:
{
  ld2x64pace    $azeros, $a2:3, $tripacked_addr+=, $stride1, 0b1011
  f16v4sisoamp  $a6:7, $azeros, $a4:5, TAMP_F16V4_E4_P2
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b0100
  f16v4sisoamp  $a6:7, $azeros, $a2:3, TAMP_F16V4_E4_P3
}
{
  // restore write pointer to correct address after writing original value
  ld2xst64pace  $a0:3, $a4:5, $tripacked_addr+=, $stride3, 0b100100
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P0
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b0100
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P1
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride1, 0b1101
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P2
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b1101
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P3
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b1101
  f16v4sisoamp  $a4:5, $a0:1, $a2:3, TAMP_F16V4_E4_P0
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b1101
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P1
}
brz             $eq2flag, LNumElemsEq1
bri             LNumElemsEq2

//------------------------------------------------------------------------------
// Code path to deal with case when partials are zero.
LNoPartialsToLoad:
tapack        $tripacked_addr, $inchan_ptr, $outchan_ptr, $outchan_ptr
brneg         $num_elems, NumElemsEq1AndEq2_Z
ld2x64pace    $a0:1, $azeros, $tripacked_addr+=, $mzero, 0b00
{
  ld2x64pace    $a0:1, $azeros, $tripacked_addr+=, $mzero, 0b00
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P0
}
{
  ld2x64pace    $a0:1, $azeros, $tripacked_addr+=, $mzero, 0b00
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P1
}
{
  ld2x64pace    $a0:1, $azeros, $tripacked_addr+=, $stride1, 0b01
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P2
}
{
  ld2x64pace    $a0:1, $azeros, $tripacked_addr+=, $mzero, 0b00
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P3
}
{
  ld2x64pace    $a0:1, $azeros, $tripacked_addr+=, $mzero, 0b00
  f16v4sisoamp  $a4:5, $a0:1, $azeros, TAMP_F16V4_E4_P0
}
{
  ld2x64pace    $a0:1, $azeros, $tripacked_addr+=, $mzero, 0b00
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P1
}

rpt $num_elems, (Loop_end_Amp_Z-Loop_start_Amp_Z)/8-1
Loop_start_Amp_Z:
  // The reads in the last pass are effectively dummy to avoid code bloat
  {
    ldst64pace    $a0:1, $a4:5, $tripacked_addr+=, $stride1, 0b0001
    f16v4sisoamp  $a4:5, $a0:1, $azeros, TAMP_F16V4_E4_P2
  }
  {
    ldst64pace    $a0:1, $a6:7, $tripacked_addr+=, $mzero, 0b0000
    f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P3
  }
  {
    ldst64pace    $a0:1, $a4:5, $tripacked_addr+=, $mzero, 0b0000
    f16v4sisoamp  $a4:5, $a0:1, $azeros, TAMP_F16V4_E4_P0
  }
  {
    ldst64pace    $a0:1, $a6:7, $tripacked_addr+=, $stride1, 0b1000
    f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P1
  }
Loop_end_Amp_Z:

{
  ldst64pace    $a0:1, $a4:5, $tripacked_addr+=, $stride1, 0b0001
  f16v4sisoamp  $a4:5, $a0:1, $azeros, TAMP_F16V4_E4_P2
}
{
  ldst64pace    $a0:1, $a6:7, $tripacked_addr+=, $mzero, 0b0000
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P3
}
{
  ldst64pace    $a0:1, $a4:5, $tripacked_addr+=, $mzero, 0b0000
  f16v4sisoamp  $a4:5, $a0:1, $azeros, TAMP_F16V4_E4_P0
}
{
  ldst64pace    $a0:1, $a6:7, $tripacked_addr+=, $stride1, 0b1000
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P1
}
bri LNumElemsEq2

// Handles the case of number of elements <=2
// stride1 at any point contains strides for both input and output. These
// may be modified to avoid overreading partials
NumElemsEq1AndEq2_Z:
// This code fragment is called if number of elements are 0, 1, or 2
add           $cmp_res, $num_elems, 1
cmpeq         $stride3, $cmp_res, $mzero
brz           $cmp_res, SetStride1NumElemsEq1_Z
add           $cmp_res, $cmp_res, 1
brneg         $cmp_res, L_end_fn
setzi         $stride1, 0

SetStride1NumElemsEq1_Z:
ld2x64pace    $a0:1, $azeros, $tripacked_addr+=, $stride3, 0b00
{
  ld2x64pace    $a0:1, $azeros, $tripacked_addr+=, $stride3, 0b00
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P0
}
{
  ld2x64pace    $a0:1, $azeros, $tripacked_addr+=, $stride3, 0b00
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P1
}
{
  ld2x64pace    $a0:1, $azeros, $tripacked_addr+=, $stride1, 0b01
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P2
}
{
  ld2x64pace    $a0:1, $azeros, $tripacked_addr+=, $stride3, 0b01
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P3
}
{
  ld2x64pace    $a0:1, $azeros, $tripacked_addr+=, $stride3, 0b01
  f16v4sisoamp  $a4:5, $a0:1, $azeros, TAMP_F16V4_E4_P0
}
{
  ld2x64pace    $a0:1, $azeros, $tripacked_addr+=, $stride3, 0b01
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P1
}
brz             $stride3, LNumElemsEq1
bri             LNumElemsEq2

.size convPartialFlattenedField, . - convPartialFlattenedField

// =============================================================================

// supervisor base is $m0 - passed to this function
#define sup_base                  m0
#define stride_s                  m1
#define conv_group_s              m1
#define invec_s                   m2
#define outvec_s                  m2
#define inchan_group_s            m3
#define partition_s               m3
#define tmem_base                 m4
#define wkr_function              m4
#define amp_group_s               m5
#define add_zero_in_stride_s      m5
#define outchan_group_s           m6
#define temp_s                    m6
#define outstride_s               m6
#define const0x80000000           m7
#define weight_vec_s              m7
#define outchan_vectors_s         m10
#define inchan_vectors_s          m9
#define weights_vectors_s         m8
#define wkr_vertex                sp

#define SUP_STACK_CALLEE_SAVE0    0  //(word)
#define SUP_STACK_CALLEE_SAVE1    4  //(word)
#define SUP_STACK_CONV_GROUP      8 //(word)
// The number of output channels per group is divided by the number of number
// of channels the AMP processes. The actual value kept is one less.
#define SUP_STACK_AMP_GROUPS      12 //(word)
#define SUP_STACK_SIZE            (SUP_STACK_AMP_GROUPS + 4)

.macro CONV_1X1 LDTYPE

.if \LDTYPE == LD64
.section .text.CODELET_NAME_64
.align 4
.globl CODELET_NAME_64
.type CODELET_NAME_64, @function
CODELET_NAME_64:

.elseif \LDTYPE == LD128
.section .text.CODELET_NAME_128
.globl CODELET_NAME_128
.type CODELET_NAME_128, @function
CODELET_NAME_128:

.else
.error "Load type not supported"
.endif
.supervisor
// Performance:
// 50 + numConvGroups * (12 +
//                       inChanGroups * (15 +
//                                       outChanGroups * (19 +
//                                                        AmpGroups * 16 + LOADCYCLES))))
// Where AMP groups = OutChansPerGroup / 8
// and LOADCYCLES = 16 for 128 bit load
//                = 32 for 64 bit load
// ----------------------------------------------------------------------------
#if defined(VECTOR_AVAIL_SCALED_PTR32)
  ldz16         $partition_s, $sup_base, SUP_PARTITION/2
#else
  ld32          $partition_s, $sup_base, SUP_PARTITION/4
#endif
  setzi         $tmem_base, TMEM_REGION0_BASE_ADDR / 4
  add           $sp, $sp, -(WKR_VERTEX_SIZE + SUP_STACK_SIZE)
  lds16         $stride_s, $sup_base, SUP_INPUT_STRIDE/2
  ldz16         $amp_group_s, $sup_base, SUP_OUTCHANS_PER_GROUP/2
  lds16         $outstride_s, $sup_base, SUP_OUTSTRIDE/2
// ----------------------------------------------------------------------------
#if defined(VECTOR_AVAIL_SCALED_PTR32)
  add           $partition_s, $partition_s, $tmem_base
  ldz16         $weights_vectors_s, $sup_base, SUP_WEIGHTS_VECTORS/2
#else
  nop           // keep nop for 6 instructions pipeline
  ld32          $weights_vectors_s, $sup_base, SUP_WEIGHTS_VECTORS/4
#endif
  st32          $m9, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CALLEE_SAVE0/4
  st32          $m10, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CALLEE_SAVE1/4
  shl           $amp_group_s, $amp_group_s, 2
  shr           $outstride_s, $outstride_s, 1
// ----------------------------------------------------------------------------
#if defined(VECTOR_AVAIL_SCALED_PTR32)
  ldz16         $inchan_vectors_s, $sup_base, SUP_INCHAN_VECTORS/2
  ldz16         $outchan_vectors_s, $sup_base, SUP_OUTCHAN_VECTORS/2
  shl           $partition_s, $partition_s, 2
  add           $weights_vectors_s, $weights_vectors_s, $tmem_base
#else
  ld32          $inchan_vectors_s, $sup_base, SUP_INCHAN_VECTORS/4
  ld32          $outchan_vectors_s, $sup_base, SUP_OUTCHAN_VECTORS/4
  nop           // keep nop for 6 instructions pipeline
  nop           // keep nop for 6 instructions pipeline
#endif
  st32          $amp_group_s,  $wkr_vertex, WKR_OUTCHANS_PER_GROUP/4
  and           $temp_s, $outstride_s, 0x3FF
// ----------------------------------------------------------------------------
  shr           $amp_group_s, $amp_group_s, (LOG2_AMP_OUTPUTS + 2)
#if defined(VECTOR_AVAIL_SCALED_PTR32)
  add           $inchan_vectors_s, $inchan_vectors_s, $tmem_base
  st32          $partition_s, $sp, WKR_PARTITION/4
  shl           $weights_vectors_s, $weights_vectors_s, 2
#else
  nop           // keep nop for 6 instructions pipeline
  st32          $partition_s, $sp, WKR_PARTITION/4
  nop           // keep nop for 6 instructions pipeline
#endif
  ldz16         $inchan_group_s, $mzero, $sup_base, SUP_NUM_INCHAN_GROUPS/2
  shl           $temp_s, $temp_s, 10
// ----------------------------------------------------------------------------
  add           $amp_group_s, $amp_group_s, -1
#if defined(VECTOR_AVAIL_SCALED_PTR32)
  shl           $inchan_vectors_s, $inchan_vectors_s, 2
  add           $outchan_vectors_s, $outchan_vectors_s, $tmem_base
#else
  nop           // keep nop for 6 instructions pipeline
  nop           // keep nop for 6 instructions pipeline
#endif
  nop
#if defined(VECTOR_AVAIL_SCALED_PTR64)
  ldz16step     $weight_vec_s, $mzero, $weights_vectors_s+=, 1
#else
  ld32step      $weight_vec_s, $mzero, $weights_vectors_s+=, 1
#endif
  or            $stride_s, $stride_s, $temp_s
// ----------------------------------------------------------------------------
  st32          $amp_group_s, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_AMP_GROUPS/4
#if defined(VECTOR_AVAIL_SCALED_PTR64)
  ldz16step     $invec_s, $mzero, $inchan_vectors_s+=, 1
#else
  ld32step      $invec_s, $mzero, $inchan_vectors_s+=, 1
#endif
#if defined(VECTOR_AVAIL_SCALED_PTR32)
  shl           $outchan_vectors_s, $outchan_vectors_s, 2
#else
  nop           // keep nop for 6 instructions pipeline
#endif
  nop
#if defined(VECTOR_AVAIL_SCALED_PTR64)
  shl           $weight_vec_s, $weight_vec_s, 3
#else
  nop           // keep nop for 6 instructions pipeline
#endif
  st32          $stride_s, $wkr_vertex, WKR_IN_OUT_STRIDES/4
// ----------------------------------------------------------------------------
  setzi         $wkr_function, convPartialFlattenedField
  ldz16         $conv_group_s, $sup_base, SUP_NUM_CONV_GROUPS_M1/2
  ldz16         $outchan_group_s, $mzero, $sup_base, SUP_NUM_OUTCHAN_GROUPS_M1/2
#if defined(VECTOR_AVAIL_SCALED_PTR64)
  // expand scaled pointer
  shl           $invec_s, $invec_s, 3
#else
  nop           // keep nop for 6 instructions pipeline
#endif
  put           $CCCSLOAD, $weight_vec_s
  nop
// ----------------------------------------------------------------------------

convGroupsLoop_\LDTYPE\():
  add           $inchan_group_s, $inchan_group_s, -1

inChanLoop_\LDTYPE\():
    st32          $invec_s, $wkr_vertex,  WKR_INCHAN_PTR/4

outChanLoop_\LDTYPE\():
#if defined(VECTOR_AVAIL_SCALED_PTR64)
      ldz16step    $weight_vec_s, $mzero, $weights_vectors_s+=, 1
      ldz16        $outvec_s, $mzero, $outchan_vectors_s, $outchan_group_s
      // expand scaled pointer
      shl          $outvec_s, $outvec_s, 3
      shl          $weight_vec_s, $weight_vec_s, 3
#else
      ld32step     $weight_vec_s, $mzero, $weights_vectors_s+=, 1
      ld32         $outvec_s, $mzero, $outchan_vectors_s, $outchan_group_s
#endif
      ld32         $amp_group_s, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_AMP_GROUPS/4

AmpGroupLoop_\LDTYPE\():
        // must wait for all workers to load CWEI as it is shared
        sync          TEXCH_SYNCZONE_LOCAL
.if \LDTYPE == LD128

        ld128putcs    0
        ld128putcs    2
        ld128putcs    4
        ld128putcs    6
        ld128putcs    8
        ld128putcs    10
        ld128putcs    12
        ld128putcs    14
        ld128putcs    16
        ld128putcs    18
        st32          $outvec_s, $mzero, $wkr_vertex, WKR_OUTCHAN_PTR/4
        ld128putcs    20
        ld128putcs    22
        ld128putcs    24
        ld128putcs    26
        ld128putcs    28
        ld128putcs    30

.elseif \LDTYPE == LD64

        ld64putcs     0
        ld64putcs     1
        ld64putcs     2
        ld64putcs     3
        ld64putcs     4
        ld64putcs     5
        ld64putcs     6
        ld64putcs     7
        ld64putcs     8
        ld64putcs     9
        ld64putcs     10
        ld64putcs     11
        ld64putcs     12
        ld64putcs     13
        ld64putcs     14
        ld64putcs     15
        ld64putcs     16
        ld64putcs     17
        ld64putcs     18
        ld64putcs     19
        st32          $outvec_s, $mzero, $wkr_vertex, WKR_OUTCHAN_PTR/4
        ld64putcs     20
        ld64putcs     21
        ld64putcs     22
        ld64putcs     23
        ld64putcs     24
        ld64putcs     25
        ld64putcs     26
        ld64putcs     27
        ld64putcs     28
        ld64putcs     29
        ld64putcs     30
        ld64putcs     31

.else
.error "Weight load type not supported"
.endif
        add           $outvec_s, $outvec_s, AMP_OUTPUTS * SIZEOF_FLOAT
        runall        $wkr_function, $wkr_vertex, 0
#if WORKER_REG_STATE_RETAINED == 1
        setzi         $wkr_function, convPartialFlattenedFieldStateRetained
#endif
        // increment output vector pointer for the AMP loop
        brnzdec $amp_group_s, AmpGroupLoop_\LDTYPE\()
      ld32          $add_zero_in_stride_s, $wkr_vertex, WKR_IN_OUT_STRIDES/4
      put           $CCCSLOAD, $weight_vec_s
      brnzdec       $outchan_group_s, outChanLoop_\LDTYPE\()

      // Use the MSB of stride which is unused to flag zeroing of partials
      or            $const0x80000000, $mzero, 0x80000000
      or            $add_zero_in_stride_s, $add_zero_in_stride_s, 0x80000000      
#if defined(VECTOR_AVAIL_SCALED_PTR64)
    ldz16step     $invec_s, $mzero, $inchan_vectors_s+=, 1
#else
    ld32step      $invec_s, $mzero, $inchan_vectors_s+=, 1
#endif
    ldz16         $outchan_group_s, $mzero, $sup_base, SUP_NUM_OUTCHAN_GROUPS_M1/2
    // cannot write to worker vertex state until all workers have finished
    // processing the output channel group
    sync          TEXCH_SYNCZONE_LOCAL
    st32          $add_zero_in_stride_s, $wkr_vertex, WKR_IN_OUT_STRIDES/4
#if defined(VECTOR_AVAIL_SCALED_PTR64)
    // expand scaled pointer
    shl           $invec_s, $invec_s, 3
#endif
    brnzdec       $inchan_group_s, inChanLoop_\LDTYPE\()
  
  // This should clear the msb
  xor           $add_zero_in_stride_s, $add_zero_in_stride_s, $const0x80000000
#if defined(VECTOR_AVAIL_SCALED_PTR64)
  ldz16step     $mzero, $mzero, $outchan_vectors_s+=, $outchan_group_s
  add           $outchan_vectors_s, $outchan_vectors_s, 2
#else
  ld32step      $mzero, $mzero, $outchan_vectors_s+=, $outchan_group_s
  add           $outchan_vectors_s, $outchan_vectors_s, 4
#endif
  ldz16         $inchan_group_s, $mzero, $sup_base, SUP_NUM_INCHAN_GROUPS/2
  st32          $add_zero_in_stride_s, $wkr_vertex, WKR_IN_OUT_STRIDES/4
  brnzdec       $conv_group_s, convGroupsLoop_\LDTYPE\()

// restore stack
ld32          $m9, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CALLEE_SAVE0/4
ld32          $m10, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CALLEE_SAVE1/4
add           $sp, $sp, WKR_VERTEX_SIZE + SUP_STACK_SIZE
br            $lr

.if \LDTYPE == LD64
.size CODELET_NAME_64, . - CODELET_NAME_64
.elseif \LDTYPE == LD128
.size CODELET_NAME_128, . - CODELET_NAME_128
.endif
.endm

// =============================================================================

// Instantiate codelets
CONV_1X1 LD128
CONV_1X1 LD64

// =============================================================================
#endif // #ifdef __IPU__
// =============================================================================
