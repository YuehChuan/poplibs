// Copyright (c) Graphcore Ltd, All rights reserved.
//
// Contains functions to calculate partials for convolution. Partials and Output
// are used interchangebly in this file. Each worker may process part of a
// contiguous field. This is  done by setting up a partition which contains an
// input offset in the input channel group, an output offset in the output field
// and the number of field elements to process.
//

#ifdef __IPU__

#include "poplar/AvailableVTypes.h"
#include "poplibs_support/TileConstants.hpp"
#define ZAACC_BITMASK (CSR_W_FP_CLR__ZAACC__MASK << CSR_W_FP_CLR__ZAACC__SHIFT)


// =============================================================================

// The type of weight loading supported
#define LD128                     0  // use ld128putcs
#define LD64                      1  // use ld64putcs

// =============================================================================

#define CODELET_NAME_128 __runCodelet_poplin__ConvPartial1x1Out___half_float_true_true
#define CODELET_NAME_64 __runCodelet_poplin__ConvPartial1x1Out___half_float_true_false

// =============================================================================

// Number of outputs generated by AMP
#define AMP_OUTPUTS               8
#define LOG2_AMP_OUTPUTS          3
#define SIZEOF_FLOAT              4
#define SIZEOF_HALF               2

// The number of input channel groups is fixed to 16
#define NUM_INCHAN_GROUPS         16

// =============================================================================

//// Supervisor vertex state
////
#if defined(VECTOR_AVAIL_SCALED_PTR32)
// Pointer to input channel group deltas
#define SUP_INCHAN_VECTORS        0  // scaled32, ushort
// Pointer to weights vectors
#define SUP_WEIGHTS_VECTORS       2  // scaled32, ushort
// Pointer to partials
#define SUP_OUTCHAN_VECTORS       4  // scaled32, ushort
// Pointer to worker partition table
#define SUP_PARTITION             6  // scaled32, ushort
// Number of convolution groups
#define SUP_NUM_CONV_GROUPS_M1    8   // short
// Number of contiguous output channel group fields. Value is 1 less.
#define SUP_NUM_OUTCHAN_GROUPS_M1 10  // short
// Number of contiguous input channel group fields
#define SUP_NUM_INCHAN_GROUPS     12  // short
// input and output strides
#define SUP_INPUT_STRIDE          14  // short
// number of output channels per group
#define SUP_OUTCHANS_PER_GROUP    16  // short
// Processed out stride: the value depends on whether output is flipped or
// not. This vertex does not actually support striding but this field is used
// to directly create a stride for the flipped output
#define SUP_OUTSTRIDE             18 // short

#else
// Pointer to input channel group deltas
#define SUP_INCHAN_VECTORS        0  // one_ptr, unsigned
// Pointer to weights vectors
#define SUP_WEIGHTS_VECTORS       4  // one_ptr, unsigned
// Pointer to partials
#define SUP_OUTCHAN_VECTORS       8  // one_ptr, unsigned
// Pointer to worker partition table
#define SUP_PARTITION             12  // one_ptr, unsigned
// Number of convolution groups
#define SUP_NUM_CONV_GROUPS_M1    16  // short
// Number of contiguous output channel group fields. Value is 1 less.
#define SUP_NUM_OUTCHAN_GROUPS_M1 18  // short
// Number of contiguous input channel group fields
#define SUP_NUM_INCHAN_GROUPS     20  // short
// input and output strides
#define SUP_INPUT_STRIDE          22  // short
// number of output channels per group
#define SUP_OUTCHANS_PER_GROUP    24  // short
// Processed out stride: the value depends on whether output is flipped or
// not. This vertex does not actually support striding but this field is used
// to directly create a stride for the flipped output
#define SUP_OUTSTRIDE             26 // short

#endif // #if defined(VECTOR_AVAIL_SCALED_PTR32)

// =============================================================================

//// Vertex state shared between workers (Worker vertex state is allocated
//// on supervisor stack and along with stack space used by supervisor must be
//// a multiple of 8 bytes)
////
// Pointer to input channel group field
#define WKR_INCHAN_PTR            0    // word
// Pointer to output/partial channel group field
#define WKR_OUTCHAN_PTR           4    // word
// Input stride
#define WKR_IN_OUT_STRIDES        8   // word
// If flag is non-zero, partials must be zeroed
#define WKR_ZERO_FLAG             12   // word
// Gap to be left in bytes after all partials are written for a field position
#define WKR_OUTCHANS_PER_GROUP    16   // word
#define WKR_PARTITION             20
#define WKR_VERTEX_SIZE           24   // bytes

// =============================================================================

.section ".text.convPartialFlattenedField", "ax"
.type convPartialFlattenedField, @function
.align 8
.worker
// worker code:
//        num_field_pos = 0
//                      26
//        num_field_pos == 1
//                      41 + (2 + 4) * first_input_group
//                   num_field_pos == 2
//                      42 + (2 + 4 * 2) * first_input_group
//                   num_field_pos >= 3
//                      43  + (2 + 4 * num_field_pos) * first_input_group
//                          + (num_field_pos - 3) * 4
//
// first_in_group is set to a 1 for the first input channel group of every
// convolution, in which case no partial is loaded.
//

// Total:
convPartialFlattenedFieldAligned:
//nop
convPartialFlattenedField:

#define inchan_ptr                m0
#define outchan_ptr               m1
#define partition_w               m2
#define num_outchans_per_groupx4  m3
#define zero_outchan              m4
#define num_elems                 m5
#define stride1                   m6
#define cmp_res                   m7
// $inchan_ptr_iter and $outchan_ptr_iter must be the same as $tripacked_addr pair
#define inchan_ptr_iter           m8
#define out_off                   m9
#define outchan_ptr_iter          m9
#define tripacked_addr            m8:9
#define stride3                   m10
#define wkr_id                    m11
#define in_off                    m11

{
get           $wkr_id, $WSR
setzi         $a0, ZAACC_BITMASK
}
{
and           $wkr_id, $wkr_id, CSR_W_WSR__CTXTID_M1__MASK
uput          $FP_CLR, $a0
}
// each partition is 3 halves
mul           $wkr_id, $wkr_id, 6
ld32          $partition_w, $mvertex_base, WKR_PARTITION/4

ld32          $inchan_ptr, $mvertex_base, WKR_INCHAN_PTR/4
ld32          $outchan_ptr, $mvertex_base, WKR_OUTCHAN_PTR/4

ld32          $num_outchans_per_groupx4, $mvertex_base, WKR_OUTCHANS_PER_GROUP/4
ld32          $zero_outchan, $mvertex_base, WKR_ZERO_FLAG/4

ldz16step     $out_off, $wkr_id, $partition_w+=, 1
mul           $out_off, $out_off, $num_outchans_per_groupx4
ldz16step     $num_elems, $wkr_id, $partition_w+=, 1
ldz16step     $in_off, $wkr_id, $partition_w+=, 1
mul           $in_off, $in_off, NUM_INCHAN_GROUPS * SIZEOF_HALF
ld32          $stride1, $mvertex_base, WKR_IN_OUT_STRIDES/4

add           $outchan_ptr, $outchan_ptr, $out_off

// The two input pointers will be retained even though store pointer is incremented
tapack        $tripacked_addr, $inchan_ptr, $outchan_ptr, $outchan_ptr

brz           $zero_outchan, Amp_start
rpt           $num_elems, (LZeroLoopEnd - LZeroLoopBegin)/8 - 1

LZeroLoopBegin:
  {
    st64pace      $azeros,          $tripacked_addr+=, $mzero, 0b00
    fnop
  }
  {
    st64pace      $azeros,          $tripacked_addr+=, $mzero, 0b00
    fnop
  }
  {
    st64pace      $azeros,          $tripacked_addr+=, $mzero, 0b00
    fnop
  }
  {
    st64pace      $azeros,          $tripacked_addr+=, $stride1, 0b10
    fnop
  }
LZeroLoopEnd:
Amp_start:

add           $inchan_ptr, $inchan_ptr, $in_off
add           $num_elems, $num_elems, -3

// can do a ld128 as partials are always in interleaved memory and the
// second pointer increment doesn not affect the store pointer
ld128step     $a0:3, $mzero, $outchan_ptr+=, 1
{
  // Get compact representation of physical addresses
  tapack        $tripacked_addr, $inchan_ptr, $outchan_ptr, $outchan_ptr_iter
  f16v4sisoamp  $a6:7, $azeros, $a0:1, TAMP_F16V4_E4_P0
}
{
  ld2x64pace    $azeros, $a2:3, $tripacked_addr+=, $mzero, 0b0011
  f16v4sisoamp  $a6:7, $azeros, $a2:3, TAMP_F16V4_E4_P1
}
{
  brneg         $num_elems, NumElemsEq1AndEe2
  fnop
}
{
  ld2x64pace    $azeros, $a2:3, $tripacked_addr+=, $stride1, 0b1011
  f16v4sisoamp  $a6:7, $azeros, $a2:3, TAMP_F16V4_E4_P2
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $mzero, 0b0000
  f16v4sisoamp  $a6:7, $azeros, $a2:3, TAMP_F16V4_E4_P3
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $mzero, 0b0000
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P0
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $mzero, 0b0000
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P1
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride1, 0b1001
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P2
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $mzero, 0b0000
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P3
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $mzero, 0b0000
  f16v4sisoamp  $a4:5, $a0:1, $a2:3, TAMP_F16V4_E4_P0
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $mzero, 0b0000
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P1
}

rpt $num_elems, (Loop_end_Amp-Loop_start_Amp)/8-1
Loop_start_Amp:
  // The reads in the last pass are effectively dummy to avoid code bloat
  {
    ld2xst64pace  $a0:3, $a4:5, $tripacked_addr+=, $stride1, 0b001001
    f16v4sisoamp  $a4:5, $a0:1, $a2:3, TAMP_F16V4_E4_P2
  }
  {
    ld2xst64pace  $a0:3, $a6:7, $tripacked_addr+=, $mzero, 0b000000
    f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P3
  }
  {
    ld2xst64pace  $a0:3, $a4:5, $tripacked_addr+=, $mzero, 0b000000
    f16v4sisoamp  $a4:5, $a0:1, $a2:3, TAMP_F16V4_E4_P0
  }
  {
    ld2xst64pace  $a0:3, $a6:7, $tripacked_addr+=, $stride1, 0b100000
    f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P1
  }
Loop_end_Amp:

{
  ld2xst64pace  $a0:3, $a4:5, $tripacked_addr+=, $stride1, 0b001101
  f16v4sisoamp  $a4:5, $a0:1, $a2:3, TAMP_F16V4_E4_P2
}
{
  ldst64pace    $a0:1, $a6:7, $tripacked_addr+=, $mzero, 0b0000
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P3
}
{
  ldst64pace    $a0:1, $a4:5, $tripacked_addr+=, $mzero, 0b0000
  f16v4sisoamp  $a4:5, $a0:1, $azeros, TAMP_F16V4_E4_P0
}
{
  ldst64pace    $a0:1, $a6:7, $tripacked_addr+=, $stride1, 0b1000
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P1
}
LNumElemsEq2:
{
  ldst64pace    $a0:1, $a4:5, $tripacked_addr+=, $mzero, 0b000
  f16v4sisoamp  $a4:5, $a0:1, $azeros, TAMP_F16V4_E4_P2
}
{
  st64pace      $a6:7, $tripacked_addr+=, $mzero, 0b00
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P3
}
{
  st64pace      $a4:5, $tripacked_addr+=, $mzero, 0b00
  f16v4sisoamp  $a4:5, $a0:1, $azeros, TAMP_F16V4_E4_P0
}
{
  st64pace      $a6:7, $tripacked_addr+=, $stride1, 0b10
  f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P1
}

LNumElemsEq1:

// This may need to change if partials for the next loop could be loaded
// with the store of old results
{
  st64pace      $a4:5,          $tripacked_addr+=, $mzero, 0b00
  f16v4sisoamp  $a4:5, $azeros, $azeros, TAMP_F16V4_E4_P2
}
{
  st64pace      $a6:7,          $tripacked_addr+=, $mzero, 0b00
  f16v4sisoamp  $a6:7, $azeros, $azeros, TAMP_F16V4_E4_P3
}
st64pace      $a4:5,          $tripacked_addr+=, $mzero, 0b00
st64pace      $a6:7,          $tripacked_addr+=, $mzero, 0b00

L_end_fn:
exitz         $m15

// Handles the case of number of elements <=2
// stride1 at any point contains strides for both input and output. These
// may be modified to avoid overreading partials
NumElemsEq1AndEe2:
// This code fragment is called if number of elements are 0, 1, or 2
add           $cmp_res, $num_elems, 1
cmpeq         $stride3, $cmp_res, $mzero
brz           $cmp_res, SetStride1NumElemsEq1
add           $cmp_res, $cmp_res, 1
brneg         $cmp_res, L_end_fn
setzi         $stride1, 0

SetStride1NumElemsEq1:
{
  ld2x64pace    $azeros, $a2:3, $tripacked_addr+=, $stride1, 0b1011
  f16v4sisoamp  $a6:7, $azeros, $a2:3, TAMP_F16V4_E4_P2
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b0100
  f16v4sisoamp  $a6:7, $azeros, $a2:3, TAMP_F16V4_E4_P3
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b0100
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P0
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b0100
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P1
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride1, 0b1101
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P2
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b1001
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P3
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b1001
  f16v4sisoamp  $a4:5, $a0:1, $a2:3, TAMP_F16V4_E4_P0
}
{
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b1001
  f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P1
}
brz             $stride3, LNumElemsEq1
bri             LNumElemsEq2


.size convPartialFlattenedField, . - convPartialFlattenedField

// =============================================================================

// supervisor base is $m0 - passed to this function
#define sup_base                  m0
#define stride_s                  m1
#define conv_group_s              m1
#define invec_s                   m2
#define outvec_s                  m2
#define inchan_group_s            m3
#define partition_s               m3
#define tmem_base                 m4
#define wkr_function              m4
#define amp_group_s               m5
#define outchan_group_s           m6
#define temp_s                    m6
#define outstride_s               m6
#define weight_vec_s              m7
#define outchan_vectors_s         m10
#define inchan_vectors_s          m9
#define weights_vectors_s         m8
#define wkr_vertex                sp

#define SUP_STACK_CALLEE_SAVE0    0  //(word)
#define SUP_STACK_CALLEE_SAVE1    4  //(word)
#define SUP_STACK_CONV_GROUP      8 //(word)
// The number of output channels per group is divided by the number of number
// of channels the AMP processes. The actual value kept is one less.
#define SUP_STACK_AMP_GROUPS      12 //(word)
#define SUP_STACK_SIZE            (SUP_STACK_AMP_GROUPS + 4)

.macro CONV_1X1 LDTYPE

.if \LDTYPE == LD64
.section .text.CODELET_NAME_64
.align 4
.globl CODELET_NAME_64
.type CODELET_NAME_64, @function
CODELET_NAME_64:

.elseif \LDTYPE == LD128
.section .text.CODELET_NAME_128
.globl CODELET_NAME_128
.type CODELET_NAME_128, @function
CODELET_NAME_128:

.else
.error "Load type not supported"
.endif
.supervisor
// Performance:
// 50 + numConvGroups * (11 +
//                       inChanGroups * (13 +
//                                       outChanGroups * (11 +
//                                                        AmpGroups * 16 + LOADCYCLES))))
// Where AMP groups = OutChansPerGroup / 8
// and LOADCYCLES = 16 for 128 bit load
//                = 32 for 64 bit load
// ----------------------------------------------------------------------------
#if defined(VECTOR_AVAIL_SCALED_PTR32)
  ldz16         $partition_s, $sup_base, SUP_PARTITION/2
#else
  ld32          $partition_s, $sup_base, SUP_PARTITION/4
#endif  
  setzi         $tmem_base, TMEM_REGION0_BASE_ADDR / 4
  add           $sp, $sp, -(WKR_VERTEX_SIZE + SUP_STACK_SIZE)
  lds16         $stride_s, $sup_base, SUP_INPUT_STRIDE/2
  ldz16         $amp_group_s, $sup_base, SUP_OUTCHANS_PER_GROUP/2
  lds16         $outstride_s, $sup_base, SUP_OUTSTRIDE/2
// ----------------------------------------------------------------------------
#if defined(VECTOR_AVAIL_SCALED_PTR32)
  add           $partition_s, $partition_s, $tmem_base
  ldz16         $weights_vectors_s, $sup_base, SUP_WEIGHTS_VECTORS/2
#else
  nop           // keep nop for 6 instructions pipeline
  ld32          $weights_vectors_s, $sup_base, SUP_WEIGHTS_VECTORS/4
#endif   
  st32          $m9, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CALLEE_SAVE0/4
  st32          $m10, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CALLEE_SAVE1/4
  shl           $amp_group_s, $amp_group_s, 2
  shr           $outstride_s, $outstride_s, 1
// ----------------------------------------------------------------------------  
#if defined(VECTOR_AVAIL_SCALED_PTR32) 
  ldz16         $inchan_vectors_s, $sup_base, SUP_INCHAN_VECTORS/2
  ldz16         $outchan_vectors_s, $sup_base, SUP_OUTCHAN_VECTORS/2
  shl           $partition_s, $partition_s, 2
  add           $weights_vectors_s, $weights_vectors_s, $tmem_base
#else
  ld32          $inchan_vectors_s, $sup_base, SUP_INCHAN_VECTORS/4
  ld32          $outchan_vectors_s, $sup_base, SUP_OUTCHAN_VECTORS/4
  nop           // keep nop for 6 instructions pipeline
  nop           // keep nop for 6 instructions pipeline
#endif
  st32          $amp_group_s,  $wkr_vertex, WKR_OUTCHANS_PER_GROUP/4
  and           $temp_s, $outstride_s, 0x3FF
// ----------------------------------------------------------------------------  
  shr           $amp_group_s, $amp_group_s, (LOG2_AMP_OUTPUTS + 2)
#if defined(VECTOR_AVAIL_SCALED_PTR32)   
  add           $inchan_vectors_s, $inchan_vectors_s, $tmem_base
  st32          $partition_s, $sp, WKR_PARTITION/4
  shl           $weights_vectors_s, $weights_vectors_s, 2
#else
  nop           // keep nop for 6 instructions pipeline
  st32          $partition_s, $sp, WKR_PARTITION/4
  nop           // keep nop for 6 instructions pipeline
#endif  
  ldz16         $inchan_group_s, $mzero, $sup_base, SUP_NUM_INCHAN_GROUPS/2
  shl           $temp_s, $temp_s, 10
// ----------------------------------------------------------------------------  
  add           $amp_group_s, $amp_group_s, -1
#if defined(VECTOR_AVAIL_SCALED_PTR32)  
  shl           $inchan_vectors_s, $inchan_vectors_s, 2
  add           $outchan_vectors_s, $outchan_vectors_s, $tmem_base
#else
  nop           // keep nop for 6 instructions pipeline
  nop           // keep nop for 6 instructions pipeline
#endif
  nop
#if defined(VECTOR_AVAIL_SCALED_PTR64)   
  ldz16step     $weight_vec_s, $mzero, $weights_vectors_s+=, 1
#else
  ld32step      $weight_vec_s, $mzero, $weights_vectors_s+=, 1
#endif  
  or            $stride_s, $stride_s, $temp_s
// ----------------------------------------------------------------------------  
  st32          $amp_group_s, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_AMP_GROUPS/4
#if defined(VECTOR_AVAIL_SCALED_PTR64)  
  ldz16step     $invec_s, $mzero, $inchan_vectors_s+=, 1
#else
  ld32step      $invec_s, $mzero, $inchan_vectors_s+=, 1
#endif  
#if defined(VECTOR_AVAIL_SCALED_PTR32)   
  shl           $outchan_vectors_s, $outchan_vectors_s, 2
#else
  nop           // keep nop for 6 instructions pipeline
#endif  
  nop
#if defined(VECTOR_AVAIL_SCALED_PTR64)  
  shl           $weight_vec_s, $weight_vec_s, 3
#else
  nop           // keep nop for 6 instructions pipeline
#endif   
  st32          $stride_s, $wkr_vertex, WKR_IN_OUT_STRIDES/4
// ----------------------------------------------------------------------------  
  setzi         $wkr_function, convPartialFlattenedField
  ldz16         $conv_group_s, $sup_base, SUP_NUM_CONV_GROUPS_M1/2
  ldz16         $outchan_group_s, $mzero, $sup_base, SUP_NUM_OUTCHAN_GROUPS_M1/2
#if defined(VECTOR_AVAIL_SCALED_PTR64) 
  // expand scaled pointer
  shl           $invec_s, $invec_s, 3
#else
  nop           // keep nop for 6 instructions pipeline
#endif  
  put           $CCCSLOAD, $weight_vec_s
  nop
// ----------------------------------------------------------------------------  

convGroupsLoop_\LDTYPE\():
  // use a non-zero value to flag that zeroing of output must be done
  // this flag will be set to zero once first input channel group is processed
  st32          $weights_vectors_s, $wkr_vertex, WKR_ZERO_FLAG/4
  add           $inchan_group_s, $inchan_group_s, -1

inChanLoop_\LDTYPE\():
    st32          $invec_s, $wkr_vertex,  WKR_INCHAN_PTR/4

outChanLoop_\LDTYPE\():
#if defined(VECTOR_AVAIL_SCALED_PTR64) 
      ldz16step    $weight_vec_s, $mzero, $weights_vectors_s+=, 1
      ldz16        $outvec_s, $mzero, $outchan_vectors_s, $outchan_group_s
      // expand scaled pointer
      shl          $outvec_s, $outvec_s, 3
      shl          $weight_vec_s, $weight_vec_s, 3
#else
      ld32step     $weight_vec_s, $mzero, $weights_vectors_s+=, 1
      ld32         $outvec_s, $mzero, $outchan_vectors_s, $outchan_group_s
#endif
      ld32         $amp_group_s, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_AMP_GROUPS/4

AmpGroupLoop_\LDTYPE\():
        // must wait for all workers to load CWEI as it is shared
        sync          TEXCH_SYNCZONE_LOCAL
.if \LDTYPE == LD128

        ld128putcs    0
        ld128putcs    2
        ld128putcs    4
        ld128putcs    6
        ld128putcs    8
        ld128putcs    10
        ld128putcs    12
        ld128putcs    14
        ld128putcs    16
        ld128putcs    18
        st32          $outvec_s, $mzero, $wkr_vertex, WKR_OUTCHAN_PTR/4
        ld128putcs    20
        ld128putcs    22
        ld128putcs    24
        ld128putcs    26
        ld128putcs    28
        ld128putcs    30

.elseif \LDTYPE == LD64

        ld64putcs     0
        ld64putcs     1
        ld64putcs     2
        ld64putcs     3
        ld64putcs     4
        ld64putcs     5
        ld64putcs     6
        ld64putcs     7
        ld64putcs     8
        ld64putcs     9
        ld64putcs     10
        ld64putcs     11
        ld64putcs     12
        ld64putcs     13
        ld64putcs     14
        ld64putcs     15
        ld64putcs     16
        ld64putcs     17
        ld64putcs     18
        ld64putcs     19
        st32          $outvec_s, $mzero, $wkr_vertex, WKR_OUTCHAN_PTR/4
        ld64putcs     20
        ld64putcs     21
        ld64putcs     22
        ld64putcs     23
        ld64putcs     24
        ld64putcs     25
        ld64putcs     26
        ld64putcs     27
        ld64putcs     28
        ld64putcs     29
        ld64putcs     30
        ld64putcs     31

.else
.error "Weight load type not supported"
.endif
        add           $outvec_s, $outvec_s, AMP_OUTPUTS * SIZEOF_FLOAT
        runall        $wkr_function, $wkr_vertex, 0
        // increment output vector pointer for the AMP loop
        brnzdec $amp_group_s, AmpGroupLoop_\LDTYPE\()

      put           $CCCSLOAD, $weight_vec_s
      brnzdec       $outchan_group_s, outChanLoop_\LDTYPE\()
#if defined(VECTOR_AVAIL_SCALED_PTR64)      
    ldz16step     $invec_s, $mzero, $inchan_vectors_s+=, 1
#else
    ld32step      $invec_s, $mzero, $inchan_vectors_s+=, 1
#endif
    ldz16         $outchan_group_s, $mzero, $sup_base, SUP_NUM_OUTCHAN_GROUPS_M1/2
    // cannot write to worker vertex state until all workers have finished
    // processing the output channel group
    sync          TEXCH_SYNCZONE_LOCAL
    st32          $mzero, $wkr_vertex, WKR_ZERO_FLAG/4
#if defined(VECTOR_AVAIL_SCALED_PTR64)      
    // expand scaled pointer
    shl           $invec_s, $invec_s, 3
#endif
    brnzdec       $inchan_group_s, inChanLoop_\LDTYPE\()

#if defined(VECTOR_AVAIL_SCALED_PTR64)
  ldz16step     $mzero, $mzero, $outchan_vectors_s+=, $outchan_group_s
#else
  ld32step      $mzero, $mzero, $outchan_vectors_s+=, $outchan_group_s
#endif  
  ldz16         $inchan_group_s, $mzero, $sup_base, SUP_NUM_INCHAN_GROUPS/2
  add           $outchan_vectors_s, $outchan_vectors_s, 2
  brnzdec       $conv_group_s, convGroupsLoop_\LDTYPE\()

// restore stack
ld32          $m9, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CALLEE_SAVE0/4
ld32          $m10, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CALLEE_SAVE1/4
add           $sp, $sp, WKR_VERTEX_SIZE + SUP_STACK_SIZE
br            $lr

.if \LDTYPE == LD64
.size CODELET_NAME_64, . - CODELET_NAME_64
.elseif \LDTYPE == LD128
.size CODELET_NAME_128, . - CODELET_NAME_128
.endif
.endm

// =============================================================================

// Instantiate codelets
CONV_1X1 LD128
CONV_1X1 LD64

// =============================================================================
#endif // #ifdef __IPU__
// =============================================================================
