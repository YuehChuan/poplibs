// Copyright (c) Graphcore Ltd, All rights reserved.
//
// Contains functions to calculate partials for convolution. Partials and Output
// are used interchangeably in this file. Each worker may process part of a
// contiguous field. This is  done by setting up a partition which contains an
// input offset in the input channel group, an output offset in the output field
// and the number of output field elements to process. Both input and output
// may be strided and the output flipped
//
// TODO: The code for the supervisor can be shared with other variants of the
//       1x1 vertex (T6535)

#ifdef __IPU__

#include "poplar/AvailableVTypes.h"
#include "poplibs_support/TileConstants.hpp"
#define ZAACC_BITMASK (CSR_W_FP_CLR__ZAACC__MASK << CSR_W_FP_CLR__ZAACC__SHIFT)


// =============================================================================

// The type of weight loading supported
#define LD128                     0  // use ld128putcs
#define LD64                      1  // use ld64putcs

// =============================================================================

#define CODELET_NAME_128 __runCodelet_poplin__ConvPartial1x1Out___half_half_true_true
#define CODELET_NAME_64 __runCodelet_poplin__ConvPartial1x1Out___half_half_true_false

// =============================================================================

// Number of outputs generated by AMP
#define AMP_OUTPUTS               8
#define LOG2_AMP_OUTPUTS          3
#define SIZEOF_FLOAT              4
#define SIZEOF_HALF               2

// The number of input channel groups is fixed to 16
#define NUM_INCHAN_GROUPS         16

// =============================================================================

//// Supervisor vertex state
////
#if defined(VECTOR_AVAIL_SCALED_PTR32)
// Pointer to input channel group deltas
#define SUP_INCHAN_VECTORS        0  // scaled32, ushort
// Pointer to weights vectors
#define SUP_WEIGHTS_VECTORS       2  // scaled32, ushort
// Pointer to partials
#define SUP_OUTCHAN_VECTORS       4  // scaled32, ushort
// Pointer to worker partition table
#define SUP_PARTITION             6  // scaled32, ushort
// Number of convolution groups
#define SUP_NUM_CONV_GROUPS_M1    8   // short
// Number of contiguous output channel group fields. Value is 1 less.
#define SUP_NUM_OUTCHAN_GROUPS_M1 10  // short
// Number of contiguous input channel group fields
#define SUP_NUM_INCHAN_GROUPS     12  // short
// input and output strides
#define SUP_INPUT_STRIDE          14  // short
// number of output channels per group
#define SUP_OUTCHANS_PER_GROUP    16  // short
// Processed out stride: the value depends on whether output is flipped or
// not. This vertex does not actually support striding but this field is used
// to directly create a stride for the flipped output
#define SUP_OUTSTRIDE             18 // short

#else
// Pointer to input channel group deltas
#define SUP_INCHAN_VECTORS        0  // one_ptr, unsigned
// Pointer to weights vectors
#define SUP_WEIGHTS_VECTORS       4  // one_ptr, unsigned
// Pointer to partials
#define SUP_OUTCHAN_VECTORS       8  // one_ptr, unsigned
// Pointer to worker partition table
#define SUP_PARTITION             12  // one_ptr, unsigned
// Number of convolution groups
#define SUP_NUM_CONV_GROUPS_M1    16  // short
// Number of contiguous output channel group fields. Value is 1 less.
#define SUP_NUM_OUTCHAN_GROUPS_M1 18  // short
// Number of contiguous input channel group fields
#define SUP_NUM_INCHAN_GROUPS     20  // short
// input and output strides
#define SUP_INPUT_STRIDE          22  // short
// number of output channels per group
#define SUP_OUTCHANS_PER_GROUP    24  // short
// Processed out stride: the value depends on whether output is flipped or
// not. This vertex does not actually support striding but this field is used
// to directly create a stride for the flipped output
#define SUP_OUTSTRIDE             26 // short

#endif // #if defined(VECTOR_AVAIL_SCALED_PTR32)

// =============================================================================

//// Vertex state shared between workers (Worker vertex state is allocated
//// on supervisor stack and along with stack space used by supervisor must be
//// a multiple of 8 bytes)
////
// Pointer to input channel group field
#define WKR_INCHAN_PTR            0    // word
// Pointer to output/partial channel group field
#define WKR_OUTCHAN_PTR           4    // word
// Input stride
#define WKR_IN_OUT_STRIDES        8   // word
// If flag is non-zero, partials must be zeroed
#define WKR_ZERO_FLAG             12   // word
// Gap to be left in bytes after all partials are written for a field position
#define WKR_OUTCHANS_PER_GROUP    16   // word
#define WKR_PARTITION             20
#define WKR_VERTEX_SIZE           24   // bytes

// =============================================================================

.section ".text.convPartialFlattenedFieldHalfHalf", "ax"
.type convPartialFlattenedFieldHalfHalf, @function
.align 8
.worker
// worker code:
//       num_field_pos = 0
//                24
//       num_field_pos == 1
//                30 + (2 + 2) * first_input_group
//       num_field_pos == 2
//                30 + (2 + 2 * 2) * first_input_group
//       num_field_pos >= 3
//                30  + (2 + 2 * num_field_pos) * first_input_group
//                    + (num_field_pos - 3) * 4
//
// first_in_group is set to a 1 for the first input channel group of every
// convolution, in which case no partial is loaded.
//
// Note: 2 extra double word reads are done for num_field_pos={2}. These are
//       guaranteed to be non-strided

// Total:
convPartialFlattenedFieldHalfHalfAligned:
nop
convPartialFlattenedFieldHalfHalf:

#define inchan_ptr                m0
#define outchan_ptr               m1
#define partition_w               m2
#define num_outchans_per_groupx2  m3
#define zero_outchan              m4
#define num_elems                 m5
#define stride1                   m6
#define stride2                   m7
// $inchan_ptr_iter and $outchan_ptr_iter must be the same as $tripacked_addr pair
#define in_off                    m8
#define inchan_ptr_iter           m8
#define out_off                   m9
#define outchan_ptr_iter          m9
#define tripacked_addr            m8:9
#define cmp_res                   m10
#define stride3                   m10
#define wkr_id                    m11

{
get           $wkr_id, $WSR
setzi         $a0, ZAACC_BITMASK
}
{
and           $wkr_id, $wkr_id, CSR_W_WSR__CTXTID_M1__MASK
uput          $FP_CLR, $a0
}
mul           $wkr_id, $wkr_id, 6
ld32          $partition_w, $mvertex_base, WKR_PARTITION/4

ld32          $inchan_ptr, $mvertex_base, WKR_INCHAN_PTR/4
ld32          $outchan_ptr, $mvertex_base, WKR_OUTCHAN_PTR/4

ld32          $num_outchans_per_groupx2, $mvertex_base, WKR_OUTCHANS_PER_GROUP/4
ld32          $zero_outchan, $mvertex_base, WKR_ZERO_FLAG/4

ldz16step     $out_off, $wkr_id, $partition_w+=, 1
mul           $out_off, $out_off, $num_outchans_per_groupx2
ldz16step     $num_elems, $wkr_id, $partition_w+=, 1
ldz16step     $in_off, $wkr_id, $partition_w+=, 1
mul           $in_off, $in_off, NUM_INCHAN_GROUPS * SIZEOF_HALF
ld32          $stride1, $mvertex_base, WKR_IN_OUT_STRIDES/4

add           $inchan_ptr_iter, $inchan_ptr, $in_off
add           $outchan_ptr_iter, $outchan_ptr, $out_off

brz           $zero_outchan, Amp_start
// The two input pointers will be retained even though store pointer is incremented
tapack        $tripacked_addr, $inchan_ptr_iter, $outchan_ptr_iter, $outchan_ptr_iter
rpt           $num_elems, (LZeroLoopEnd - LZeroLoopBegin)/8 - 1
LZeroLoopBegin:
  {
    // partials += 1
    st64pace      $azeros,          $tripacked_addr+=, $mzero, 0b00
    fnop
  }
  {
    // partials += outstride
    st64pace      $azeros,          $tripacked_addr+=, $stride1, 0b10
    fnop
  }
LZeroLoopEnd:
Amp_start:

// Check strides to use in the AMP loop. The strides to use are dependent on
// the number of elements to avoid excess strided reads
add           $num_elems, $num_elems, -3
brneg         $num_elems, SetStridesNumElemsLt3

// case of num_elems >= 3
// Stride1 = Stride2 = [0][out index][in index] are the default
mov           $stride2, $stride1
setzi         $stride3, 1

AfterStrideSet:

// stride3 is 0 if number of elements = 1 else it is 1
// This reduces extra loads of the output(partials)
// Note that when number of elements = 2, 2 extra non-strided loads are done
// In all other cases, no extra loads are done

// Get compact representation of physical addresses
tapack        $tripacked_addr, $inchan_ptr_iter, $outchan_ptr_iter, $outchan_ptr_iter

// Assumption that groups in conv is directly stored as actual value -1

// Note: dummy loads used in the code as there is no ld64pace instruction
// inchan_ptr   += 0
// partials_ptr += 1
ld2x64pace    $azeros, $a2:3, $tripacked_addr+=, $stride1, 0b0011
f16v4hihoamp  $a6, $azeros, $a2, TAMP_F16V4_E4_P0

{
  // inchan_ptr += 0
  // partials_ptr += 0         (0 for num_elems=1)
  //              += outstride (for num_elems>=2)
  ld2x64pace    $azeros, $a2:3, $tripacked_addr+=, $stride1, 0b1011
  f16v4hihoamp  $a6, $azeros, $a3, TAMP_F16V4_E4_P1
}
f16v4hihoamp  $a6, $azeros, $a2, TAMP_F16V4_E4_P2
{
  // load input += 1
  // load partials += 0 (for num_elems = 1)
  //               += 1 (for num_elems = 2)
  //               += 1 (for num_elems > 2)
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b0100
  f16v4hihoamp  $a6, $azeros, $a3, TAMP_F16V4_E4_P3
}
{
  // load input += 1
  // load partials += 0
  ld2x64pace    $a0:1, $azeros, $tripacked_addr+=, $stride3, 0b1100
  f16v4hihoamp  $a6, $a0:1, $a2, TAMP_F16V4_E4_P0
}
{
  // load input += 1
  // load partials += 0         (for num_elems = 1)
  //               += 0         (for num_elems = 2)
  //               += outstride (for num_elems > 2)
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride2, 0b1000
  f16v4hihoamp  $a6, $a0:1, $a3, TAMP_F16V4_E4_P1
}
{
  // load input += 0        (for num_elems = 1)
  //            += instride (for num_elems >= 2)
  // load partials += 0
  ld2x64pace    $a0:1, $azeros, $tripacked_addr+=, $stride2, 0b1101
  f16v4hihoamp  $a6, $a0:1, $a2, TAMP_F16V4_E4_P2
}
{
  // load input += 0       (for num_elems = 1)
  //            += 1       (for num_elems >= 2)
  // load partials += 0    (for num_elems = 0)
  //               += 1    (for num_elems >= 2)
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b0101
  f16v4hihoamp  $a6, $a0:1, $a3, TAMP_F16V4_E4_P3
}
{
  // load input += 0 (for num_elems = 1)
  //            += 1 (for num_elems >= 2)
  // load partials += 0
  ld2x64pace    $a0:1, $azeros, $tripacked_addr+=, $stride3, 0b1101
  f16v4hihoamp  $a4, $a0:1, $a2, TAMP_F16V4_E4_P0
}
{
  // load input += 0            (for num_elems = 1)
  //            += 1            (for num_elems >= 2)
  // load partials += 0         (for numelems = 1)
  //               += 0         (for numelems = 2)
  //               += outstride (for numelems > 2)
  ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride2, 0b1001
  f16v4hihoamp  $a5, $a0:1, $a3, TAMP_F16V4_E4_P1
}

// exit paths for special cases of num_elems = {1, 2}
brneg         $num_elems, JumpPaths

// repeat loop entered only if num_elems > 3
rpt $num_elems, (Loop_end_Amp-Loop_start_Amp)/8-1
Loop_start_Amp:
  {
    // load input += stride,
    // store partials += 1
    ldst64pace    $a0:1, $a4:5, $tripacked_addr+=, $stride1, 0b0001
    f16v4hihoamp  $a4, $a0:1, $a2, TAMP_F16V4_E4_P2
  }
  {
    // load input += 1,
    // load partials += 1
    ld2x64pace  $a0:1, $a2:3, $tripacked_addr+=, $stride1, 0b000000
    f16v4hihoamp  $a5, $a0:1, $a3, TAMP_F16V4_E4_P3
  }
  {
    // load input += 1
    // store partials += stride
    ldst64pace    $a0:1, $a4:5, $tripacked_addr+=, $stride1, 0b1000
    f16v4hihoamp  $a4, $a0:1, $a2, TAMP_F16V4_E4_P0
  }
  {
    // load input += 1,
    // load partials += stride
    ld2x64pace  $a0:1, $a2:3, $tripacked_addr+=, $stride1, 0b1000
    f16v4hihoamp  $a5, $a0:1, $a3, TAMP_F16V4_E4_P1
  }
Loop_end_Amp:

{
  // load input += instride
  // store partials += 0
  ldst64pace    $a0:1, $a4:5, $tripacked_addr+=, $stride1, 0b1101
  f16v4hihoamp  $a6, $a0:1, $a2, TAMP_F16V4_E4_P2
}
{
  // load input += 1, store partials += 1
  // partial stored again to use pace instruction (partials load cannot be done
  // because the read pointer is already increased by stride)
  ldst64pace    $a0:1, $a4:5, $tripacked_addr+=, $stride1, 0b0000
  f16v4hihoamp  $a7, $a0:1, $a3, TAMP_F16V4_E4_P3
}
{
  // load input += 1
  // store partials += 0
  ldst64pace    $a0:1, $a6:7, $tripacked_addr+=, $stride1, 0b1100
  f16v4hihoamp  $a4, $a0:1, $azero, TAMP_F16V4_E4_P0
}
{
  // load input += 1
  // store partials += stride
  // partial stored again to use pace instruction (partials load cannot be done
  // because the read pointer is already increased by stride)
  ldst64pace    $a0:1, $a6:7, $tripacked_addr+=, $stride1, 0b1000
  f16v4hihoamp  $a5, $a0:1, $azero, TAMP_F16V4_E4_P1
}
LNumElemsEq2:
{
  // load input += stride
  // store partials += 1
  ldst64pace    $a0:1, $a4:5, $tripacked_addr+=, $stride1, 0b0001
  f16v4hihoamp  $a4, $a0:1, $azero, TAMP_F16V4_E4_P2
}
f16v4hihoamp  $a5, $a0:1, $azero, TAMP_F16V4_E4_P3
{
  // store partials += stride
  st64pace      $a4:5, $tripacked_addr+=, $stride1, 0b10
  f16v4hihoamp  $a4, $a0:1, $azero, TAMP_F16V4_E4_P0
}
f16v4hihoamp  $a5, $a0:1, $azero, TAMP_F16V4_E4_P1

LNumElemsEq1:

// This may need to change if partials for the next loop could be loaded
// with the store of old results
{
  // store partials += 1
  st64pace      $a4:5,          $tripacked_addr+=, $stride1, 0b00
  f16v4hihoamp  $a4, $azeros, $azero, TAMP_F16V4_E4_P2
}
f16v4hihoamp  $a5, $azeros, $azero, TAMP_F16V4_E4_P3
// store partials += 0
st64pace      $a4:5,          $tripacked_addr+=, $stride1, 0b00

L_end_fn:
exitz         $m15

// Code fragment to set strides for num_elems = {0, 1, 2}
// Jumps back to main program after setting strides
SetStridesNumElemsLt3:
add           $cmp_res, $num_elems, 1
brz           $cmp_res, LStrideCheckElemsEq2
add           $cmp_res, $num_elems, 2
brneg         $cmp_res, L_end_fn
// Number of elems = 1
// Stride1 = Stride2 = [0][0][0]
mov           $stride1, $mzero
mov           $stride2, $mzero
mov           $stride3, $mzero
bri           AfterStrideSet

LStrideCheckElemsEq2:
// Number of elems = 2
// Stride1 = [0][out index][in index]
// Stride2 = [0][0][in index]
and           $stride2, $stride1, 0x3FF
setzi         $stride3, 1
bri           AfterStrideSet

// This code fragment jumps to the appropriate point in the main program for
// number of elements = {1, 2}
JumpPaths:
add           $num_elems, $num_elems, 1
brz           $num_elems, LNumElemsEq2
bri           LNumElemsEq1


.size convPartialFlattenedFieldHalfHalf, . - convPartialFlattenedFieldHalfHalf

// =============================================================================

// supervisor base is $m0 - passed to this function
#define sup_base                  m0
#define stride_s                  m1
#define conv_group_s              m1
#define invec_s                   m2
#define outvec_s                  m2
#define inchan_group_s            m3
#define partition_s               m3
#define tmem_base                 m4
#define wkr_function              m4
#define amp_group_s               m5
#define outchan_group_s           m6
#define temp_s                    m6
#define outstride_s               m6
#define weight_vec_s              m7
#define outchan_vectors_s         m10
#define inchan_vectors_s          m9
#define weights_vectors_s         m8
#define wkr_vertex                sp

#define SUP_STACK_CALLEE_SAVE0    0  //(word)
#define SUP_STACK_CALLEE_SAVE1    4  //(word)
#define SUP_STACK_CONV_GROUP      8 //(word)
// The number of output channels per group is divided by the number of number
// of channels the AMP processes. The actual value kept is one less.
#define SUP_STACK_AMP_GROUPS      12 //(word)
#define SUP_STACK_SIZE            (SUP_STACK_AMP_GROUPS + 4)

.macro CONV_1X1 LDTYPE

.if \LDTYPE == LD64
.section .text.CODELET_NAME_64
.globl CODELET_NAME_64
.type CODELET_NAME_64, @function
CODELET_NAME_64:

.elseif \LDTYPE == LD128
.section .text.CODELET_NAME_128
.globl CODELET_NAME_128
.type CODELET_NAME_128, @function
CODELET_NAME_128:

.else
.error "Load type not supported"
.endif
.supervisor
// Performance:
// 50 + numConvGroups * (11 +
//                       inChanGroups * (13 +
//                                       outChanGroups * (11 +
//                                                        AmpGroups * 16 + LOADCYCLES))))
// Where AMP groups = OutChansPerGroup / 8
// and LOADCYCLES = 16 for 128 bit load
//                = 32 for 64 bit load
// ----------------------------------------------------------------------------
#if defined(VECTOR_AVAIL_SCALED_PTR32)
  ldz16         $partition_s, $sup_base, SUP_PARTITION/2
#else
  ld32          $partition_s, $sup_base, SUP_PARTITION/4
#endif  
  setzi         $tmem_base, TMEM_REGION0_BASE_ADDR / 4
  add           $sp, $sp, -(WKR_VERTEX_SIZE + SUP_STACK_SIZE)
  lds16         $stride_s, $sup_base, SUP_INPUT_STRIDE/2
  ldz16         $amp_group_s, $sup_base, SUP_OUTCHANS_PER_GROUP/2
  lds16         $outstride_s, $sup_base, SUP_OUTSTRIDE/2
// ----------------------------------------------------------------------------
#if defined(VECTOR_AVAIL_SCALED_PTR32)
  add           $partition_s, $partition_s, $tmem_base
  ldz16         $weights_vectors_s, $sup_base, SUP_WEIGHTS_VECTORS/2
#else
  nop           // keep nop for 6 instructions pipeline
  ld32          $weights_vectors_s, $sup_base, SUP_WEIGHTS_VECTORS/4
#endif   
  st32          $m9, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CALLEE_SAVE0/4
  st32          $m10, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CALLEE_SAVE1/4
  shl           $amp_group_s, $amp_group_s, 1
  shr           $outstride_s, $outstride_s, 2
// ----------------------------------------------------------------------------  
#if defined(VECTOR_AVAIL_SCALED_PTR32) 
  ldz16         $inchan_vectors_s, $sup_base, SUP_INCHAN_VECTORS/2
  ldz16         $outchan_vectors_s, $sup_base, SUP_OUTCHAN_VECTORS/2
  shl           $partition_s, $partition_s, 2
  add           $weights_vectors_s, $weights_vectors_s, $tmem_base
#else
  ld32          $inchan_vectors_s, $sup_base, SUP_INCHAN_VECTORS/4
  ld32          $outchan_vectors_s, $sup_base, SUP_OUTCHAN_VECTORS/4
  nop           // keep nop for 6 instructions pipeline
  nop           // keep nop for 6 instructions pipeline
#endif
  st32          $amp_group_s,  $wkr_vertex, WKR_OUTCHANS_PER_GROUP/4
  and           $temp_s, $outstride_s, 0x3FF
// ----------------------------------------------------------------------------  
  shr           $amp_group_s, $amp_group_s, (LOG2_AMP_OUTPUTS + 1)
#if defined(VECTOR_AVAIL_SCALED_PTR32)   
  add           $inchan_vectors_s, $inchan_vectors_s, $tmem_base
  st32          $partition_s, $sp, WKR_PARTITION/4
  shl           $weights_vectors_s, $weights_vectors_s, 2
#else
  nop           // keep nop for 6 instructions pipeline
  st32          $partition_s, $sp, WKR_PARTITION/4
  nop           // keep nop for 6 instructions pipeline
#endif  
  ldz16         $inchan_group_s, $mzero, $sup_base, SUP_NUM_INCHAN_GROUPS/2
  shl           $temp_s, $temp_s, 10
// ----------------------------------------------------------------------------  
  add           $amp_group_s, $amp_group_s, -1
#if defined(VECTOR_AVAIL_SCALED_PTR32)  
  shl           $inchan_vectors_s, $inchan_vectors_s, 2
  add           $outchan_vectors_s, $outchan_vectors_s, $tmem_base
#else
  nop           // keep nop for 6 instructions pipeline
  nop           // keep nop for 6 instructions pipeline
#endif
  nop
#if defined(VECTOR_AVAIL_SCALED_PTR64)   
  ldz16step     $weight_vec_s, $mzero, $weights_vectors_s+=, 1
#else
  ld32step      $weight_vec_s, $mzero, $weights_vectors_s+=, 1
#endif  
  or            $stride_s, $stride_s, $temp_s
// ----------------------------------------------------------------------------  
  st32          $amp_group_s, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_AMP_GROUPS/4
#if defined(VECTOR_AVAIL_SCALED_PTR64)  
  ldz16step     $invec_s, $mzero, $inchan_vectors_s+=, 1
#else
  ld32step      $invec_s, $mzero, $inchan_vectors_s+=, 1
#endif  
#if defined(VECTOR_AVAIL_SCALED_PTR32)   
  shl           $outchan_vectors_s, $outchan_vectors_s, 2
#else
  nop           // keep nop for 6 instructions pipeline
#endif  
  nop
#if defined(VECTOR_AVAIL_SCALED_PTR64)  
  shl           $weight_vec_s, $weight_vec_s, 3
#else
  nop           // keep nop for 6 instructions pipeline
#endif   
  st32          $stride_s, $wkr_vertex, WKR_IN_OUT_STRIDES/4
// ----------------------------------------------------------------------------  
  setzi         $wkr_function, convPartialFlattenedFieldHalfHalf
  ldz16         $conv_group_s, $sup_base, SUP_NUM_CONV_GROUPS_M1/2
  ldz16         $outchan_group_s, $mzero, $sup_base, SUP_NUM_OUTCHAN_GROUPS_M1/2
#if defined(VECTOR_AVAIL_SCALED_PTR64) 
  // expand scaled pointer
  shl           $invec_s, $invec_s, 3
#else
  nop           // keep nop for 6 instructions pipeline
#endif  
  put           $CCCSLOAD, $weight_vec_s
  nop
// ----------------------------------------------------------------------------   

convGroupsLoop_\LDTYPE\():
  // use a non-zero value to flag that zeroing of output must be done
  // this flag will be set to zero once first input channel group is processed
  st32          $weights_vectors_s, $wkr_vertex, WKR_ZERO_FLAG/4
  add           $inchan_group_s, $inchan_group_s, -1

inChanLoop_\LDTYPE\():
    st32          $invec_s, $wkr_vertex,  WKR_INCHAN_PTR/4

outChanLoop_\LDTYPE\():
#if defined(VECTOR_AVAIL_SCALED_PTR64) 
      ldz16step    $weight_vec_s, $mzero, $weights_vectors_s+=, 1
      ldz16        $outvec_s, $mzero, $outchan_vectors_s, $outchan_group_s
      // expand scaled pointer
      shl          $outvec_s, $outvec_s, 3
      shl          $weight_vec_s, $weight_vec_s, 3
#else
      ld32step     $weight_vec_s, $mzero, $weights_vectors_s+=, 1
      ld32         $outvec_s, $mzero, $outchan_vectors_s, $outchan_group_s
#endif
      ld32         $amp_group_s, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_AMP_GROUPS/4

AmpGroupLoop_\LDTYPE\():
        // must wait for all workers to load CWEI as it is shared
        sync          TEXCH_SYNCZONE_LOCAL
.if \LDTYPE == LD128

        ld128putcs    0
        ld128putcs    2
        ld128putcs    4
        ld128putcs    6
        ld128putcs    8
        ld128putcs    10
        ld128putcs    12
        ld128putcs    14
        ld128putcs    16
        ld128putcs    18
        st32          $outvec_s, $mzero, $wkr_vertex, WKR_OUTCHAN_PTR/4
        ld128putcs    20
        ld128putcs    22
        ld128putcs    24
        ld128putcs    26
        ld128putcs    28
        ld128putcs    30

.elseif \LDTYPE == LD64

        ld64putcs     0
        ld64putcs     1
        ld64putcs     2
        ld64putcs     3
        ld64putcs     4
        ld64putcs     5
        ld64putcs     6
        ld64putcs     7
        ld64putcs     8
        ld64putcs     9
        ld64putcs     10
        ld64putcs     11
        ld64putcs     12
        ld64putcs     13
        ld64putcs     14
        ld64putcs     15
        ld64putcs     16
        ld64putcs     17
        ld64putcs     18
        ld64putcs     19
        st32          $outvec_s, $mzero, $wkr_vertex, WKR_OUTCHAN_PTR/4
        ld64putcs     20
        ld64putcs     21
        ld64putcs     22
        ld64putcs     23
        ld64putcs     24
        ld64putcs     25
        ld64putcs     26
        ld64putcs     27
        ld64putcs     28
        ld64putcs     29
        ld64putcs     30
        ld64putcs     31

.else
.error "Weight load type not supported"
.endif
        add           $outvec_s, $outvec_s, AMP_OUTPUTS * SIZEOF_HALF
        runall        $wkr_function, $wkr_vertex, 0
        // increment output vector pointer for the AMP loop
        brnzdec $amp_group_s, AmpGroupLoop_\LDTYPE\()

      put           $CCCSLOAD, $weight_vec_s
      brnzdec       $outchan_group_s, outChanLoop_\LDTYPE\()
#if defined(VECTOR_AVAIL_SCALED_PTR64)      
    ldz16step     $invec_s, $mzero, $inchan_vectors_s+=, 1
#else
    ld32step      $invec_s, $mzero, $inchan_vectors_s+=, 1
#endif
    ldz16         $outchan_group_s, $mzero, $sup_base, SUP_NUM_OUTCHAN_GROUPS_M1/2
    // cannot write to worker vertex state until all workers have finished
    // processing the output channel group
    sync          TEXCH_SYNCZONE_LOCAL
    st32          $mzero, $wkr_vertex, WKR_ZERO_FLAG/4
#if defined(VECTOR_AVAIL_SCALED_PTR64)      
    // expand scaled pointer
    shl           $invec_s, $invec_s, 3
#endif
    brnzdec       $inchan_group_s, inChanLoop_\LDTYPE\()

#if defined(VECTOR_AVAIL_SCALED_PTR64)
  ldz16step     $mzero, $mzero, $outchan_vectors_s+=, $outchan_group_s
  add           $outchan_vectors_s, $outchan_vectors_s, 2
#else
  ld32step      $mzero, $mzero, $outchan_vectors_s+=, $outchan_group_s
  add           $outchan_vectors_s, $outchan_vectors_s, 4
#endif 
  ldz16         $inchan_group_s, $mzero, $sup_base, SUP_NUM_INCHAN_GROUPS/2
  brnzdec       $conv_group_s, convGroupsLoop_\LDTYPE\()

// restore stack
ld32          $m9, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CALLEE_SAVE0/4
ld32          $m10, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CALLEE_SAVE1/4
add           $sp, $sp, WKR_VERTEX_SIZE + SUP_STACK_SIZE
br            $lr

.if \LDTYPE == LD64
.size CODELET_NAME_64, . - CODELET_NAME_64
.elseif \LDTYPE == LD128
.size CODELET_NAME_128, . - CODELET_NAME_128
.endif
.endm

// =============================================================================

// Instantiate codelets
CONV_1X1 LD128
CONV_1X1 LD64

// =============================================================================
#endif // #ifdef __IPU__
// =============================================================================
