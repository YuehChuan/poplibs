//
// Contains functions to calculate partials for convolution. Partials and Output
// are used interchangebly in this file. Each worker may process part of a
// contiguous field. This is  done by setting up a partition which contains an
// input offset in the input channel group, an output offset in the output field
// and the number of field elements to process.
// 

#ifdef __IPU__

#include "tileimplconsts_tommy.h"
#include "tilearch.h"

#define conv_partial_sup_contiguous __runCodelet_popconv__ConvPartial1x1Out___half_float_true

// =============================================================================

// Number of outputs generated by AMP
#define AMP_OUTPUTS               8
#define LOG2_AMP_OUTPUTS          3
#define SIZEOF_FLOAT              4
#define SIZEOF_HALF               2

// The number of input channel groups is fixed to 16
#define NUM_INCHAN_GROUPS         16

// =============================================================================

//// Supervisor vertex state
////
// Pointer to input channel group deltas
#define SUP_INCHAN_VECTORS        0  // word
// Pointer to weights vectors
#define SUP_WEIGHTS_VECTORS       4  // word
// Pointer to partials
#define SUP_OUTCHAN_VECTORS       8  // word
// Pointer to worker partition table (partition is a DELTAN vectorlist)
#define SUP_PARTITION_BASEDELTA   12   // word
// Number of convolution groups
#define SUP_NUM_CONV_GROUPS_M1    20   // word
// Number of contiguous output channel group fields. Value is 1 less.
#define SUP_NUM_OUTCHAN_GROUPS_M1 24  // word
// Number of contiguous input channel group fields. Value is 1 less.
#define SUP_NUM_INCHAN_GROUPS_M1  28  // word 
// input and output strides
#define SUP_INPUT_STRIDE          32  // word
// number of output channels per group
#define SUP_OUTCHANS_PER_GROUP    36  // word
// Processed out stride: the value depends on whether output is flipped or 
// not. This vertex does not actually support striding but this field is used
// to directly create a stride for the flipped output
#define SUP_OUTSTRIDE             40

// =============================================================================

//// Vertex state shared between workers (Worker vertex state is allocated
//// on supervisor stack and along with stack space used by supervisor must be
//// a multiple of 8 bytes)
////
// Pointer to input channel group field
#define WKR_INCHAN_PTR            0    // word
// Pointer to output/partial channel group field
#define WKR_OUTCHAN_PTR           4    // word
// Pointer to partition table. Each worker has its own pointer
#define WKR_PARTITION_LIST        8   // word
// Input stride
#define WKR_IN_OUT_STRIDES        12   // word
// If flag is non-zero, partials must be zeroed
#define WKR_ZERO_FLAG             16   // word
// Gap to be left in bytes after all partials are written for a field position
#define WKR_OUTCHANS_PER_GROUP    20   // word
// Partition table is a DELTAN vector list. This gives the base address for
// the vectorlist
#define WKR_PARTITION_BASE        24
#define WKR_VERTEX_SIZE           32   // bytes   

// =============================================================================

.section ".text.convPartialFlattenedField", "ax"
.type convPartialFlattenedField, @function
.align 8

// worker code:
//       overhead: 7 cycles if number of paritions = 0
//                 14 cycles otherwise 
//       Loop (cycles for field positions): 
//                   num_field_pos = 0
//                      7
//                   num_field_pos == 1
//                      36 + (2 + 4) * first_input_group
//                   num_field_pos == 2
//                      44 + (2 + 4 * 2) * first_input_group
//                   num_field_pos >= 3
//                      49  + (2 + 4 * num_field_pos) * first_input_group 
//                          + (num_field_pos - 3) * 4
//                   
// first_in_group is set to a 1 for the first input channel group of every
// convolution, in which case no partial is loaded.
//
// Note: 3 extra double word reads are done for num_field_pos={1,2}. These are
//       guaranteed to be non-strided

// Total:  
convPartialFlattenedFieldAligned:
nop
convPartialFlattenedField:

#define inchan_ptr                m0
#define outchan_ptr               m1
#define partition_w               m2
#define in_off                    m3
#define inchan_ptr_iter           m3
#define out_off                   m4
#define outchan_ptr_iter          m4
#define num_elems                 m5
#define temp_w                    m5
#define stride1                   m6
#define stride2                   m7
#define zero_outchan              m8
#define num_outchans_per_groupx4  m8
#define tripacked_addr            m8:9  
#define cmp_res                   m10
#define stride3                   m10
#define num_partitions            m11
#define wkr_id                    m11

get           $wkr_id, $WSR
and           $wkr_id, $wkr_id, CSR_W_WSR__CTXTID_M1__MASK

ld32          $partition_w, $mvertex_base, WKR_PARTITION_LIST/4

// Point to entry in partition for this context
ld32          $partition_w, $partition_w, $wkr_id
// partition contains packed offset (18 bits), number of partitions x 3(14 bits)
shr           $num_partitions, $partition_w, 18
brz           $num_partitions, L_end_fn

ld32          $inchan_ptr, $mvertex_base, WKR_INCHAN_PTR/4
ld32          $outchan_ptr, $mvertex_base, WKR_OUTCHAN_PTR/4

// A partition is 3 entries (input offset, output offset and field size)
// The following gives num_partitions/3 - 1 for values
// [3:3:2^14-1]. The case of zero is handled above
mul           $num_partitions, $num_partitions, 21845
shr           $num_partitions, $num_partitions, 16

shl           $partition_w, $partition_w, 14
shr           $partition_w, $partition_w, 14
ld32          $temp_w, $mvertex_base, WKR_PARTITION_BASE/4
add           $partition_w, $temp_w, $partition_w

Loop_outer:
  ldz16step     $out_off, $mzero, $partition_w+=, 1
  ld32          $num_outchans_per_groupx4, $mvertex_base, WKR_OUTCHANS_PER_GROUP/4
  mul           $out_off, $out_off, $num_outchans_per_groupx4
  ldz16step     $num_elems, $mzero, $partition_w+=, 1 
  ldz16step     $in_off, $mzero, $partition_w+=, 1
  brz           $num_elems, Loop_outer_end
  mul           $in_off, $in_off, NUM_INCHAN_GROUPS * SIZEOF_HALF
  ld32          $stride1, $mvertex_base, WKR_IN_OUT_STRIDES/4

  // Check strides to use in the AMP loop. The strides to use are dependent on
  // the number of elements to avoid excess strided reads
  // Stride1 = Stride2 = [0][out index][in index] are the default
  mov           $stride2, $stride1

  cmpeq         $cmp_res, $num_elems, 2
  brz           $cmp_res, LStrideCheckElemsEq1

  // Number of elems = 2
  // Stride1 = [0][out index][in index]
  // Stride2 = [0][0][in index]
  and           $stride2, $stride1, 0x3FF

LStrideCheckElemsEq1:
  cmpeq         $cmp_res, $num_elems, 1
  brz           $cmp_res, LStrideCheckElemsGt2

  // Number of elems = 1
  // Stride1 = Stride2 = [0][0][0]
  mov           $stride1, $mzero
  mov           $stride2, $mzero

LStrideCheckElemsGt2:
  // stride3 is 0 if number of elements = 1 else it is 1
  // This avoids 6 extra loads of the output(partials)
  // Note that when number of elements = 2, 3 extra non-strided loads are done 
  // In all other cases, no extra loads are done
  cmpeq         $stride3, $cmp_res, 0

  add           $inchan_ptr_iter, $inchan_ptr, $in_off
  add           $outchan_ptr_iter, $outchan_ptr, $out_off
  ld32          $zero_outchan, $mvertex_base, WKR_ZERO_FLAG/4

  brz           $zero_outchan, Amp_start
  tapack        $tripacked_addr, $mzero, $mzero, $outchan_ptr_iter
  rpt $num_elems, (LZeroLoopEnd - LZeroLoopBegin)/8 - 1
LZeroLoopBegin:
    {
      st64pace      $azeros,          $tripacked_addr+=, $mzero, 0b00
      fnop
    }
    {
      st64pace      $azeros,          $tripacked_addr+=, $mzero, 0b00
      fnop
    }
    {
      st64pace      $azeros,          $tripacked_addr+=, $mzero, 0b00      
      fnop
    }
    {
      st64pace      $azeros,          $tripacked_addr+=, $stride1, 0b10
      fnop
    }       
LZeroLoopEnd:
Amp_start:

  // Get compact representation of physical addresses
  tapack        $tripacked_addr, $inchan_ptr_iter, $outchan_ptr_iter, $outchan_ptr_iter

  // Assumption that groups in conv is directly stored as actual vaue -1
  ld2x64pace    $azeros, $a2:3, $tripacked_addr+=, $stride1, 0b0011
  {
    ld2x64pace    $azeros, $a2:3, $tripacked_addr+=, $stride1, 0b0011
    f16v4sisoamp  $a6:7, $azeros, $a2:3, TAMP_F16V4_E4_P0
  }
  {
    ld2x64pace    $azeros, $a2:3, $tripacked_addr+=, $stride1, 0b0011
    f16v4sisoamp  $a6:7, $azeros, $a2:3, TAMP_F16V4_E4_P1
  }
  {
    ld2x64pace    $azeros, $a2:3, $tripacked_addr+=, $stride1, 0b1011
    f16v4sisoamp  $a6:7, $azeros, $a2:3, TAMP_F16V4_E4_P2
  }
  {
    ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b0100
    f16v4sisoamp  $a6:7, $azeros, $a2:3, TAMP_F16V4_E4_P3
  }
  {
    ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b0100
    f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P0
  }
  {
    ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b0100
    f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P1
  }
  {
    ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride2, 0b1001
    f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P2
  }
  {
    ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b0101
    f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P3
  }
  {
    ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b0101
    f16v4sisoamp  $a4:5, $a0:1, $a2:3, TAMP_F16V4_E4_P0
  }
  {
    ld2x64pace    $a0:1, $a2:3, $tripacked_addr+=, $stride3, 0b0101
    f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P1
  }

  // exit paths for special cases of num_elems = {1, 2}
  add           $num_elems, $num_elems, -1
  brz           $num_elems, LNumElemsEq1

  add           $num_elems, $num_elems, -2
  brneg         $num_elems, LNumElemsEq2

  {
    rpt $num_elems, (Loop_end_Amp-Loop_start_Amp)/8-1
    fnop
  }
Loop_start_Amp:
    // The reads in the last pass are effectively dummy to avoid code bloat
    {
      ld2xst64pace  $a0:3, $a4:5, $tripacked_addr+=, $stride1, 0b001001
      f16v4sisoamp  $a4:5, $a0:1, $a2:3, TAMP_F16V4_E4_P2
    }
    {
      ld2xst64pace  $a0:3, $a6:7, $tripacked_addr+=, $stride1, 0b000000
      f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P3
    }
    {
      ld2xst64pace  $a0:3, $a4:5, $tripacked_addr+=, $stride1, 0b000000
      f16v4sisoamp  $a4:5, $a0:1, $a2:3, TAMP_F16V4_E4_P0
    }
    {
      ld2xst64pace  $a0:3, $a6:7, $tripacked_addr+=, $stride1, 0b100000
      f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P1
    }
Loop_end_Amp:

  {
    ld2xst64pace  $a0:3, $a4:5, $tripacked_addr+=, $stride1, 0b001101
    f16v4sisoamp  $a4:5, $a0:1, $a2:3, TAMP_F16V4_E4_P2
  }
  {
    ldst64pace    $a0:1, $a6:7, $tripacked_addr+=, $stride1, 0b0000
    f16v4sisoamp  $a6:7, $a0:1, $a2:3, TAMP_F16V4_E4_P3
  }
  {
    ldst64pace    $a0:1, $a4:5, $tripacked_addr+=, $stride1, 0b0000
    f16v4sisoamp  $a4:5, $a0:1, $azeros, TAMP_F16V4_E4_P0
  }
  {
    ldst64pace    $a0:1, $a6:7, $tripacked_addr+=, $stride1, 0b1000
    f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P1
  }
LNumElemsEq2:
  {
    ldst64pace    $a0:1, $a4:5, $tripacked_addr+=, $stride1, 0b0001
    f16v4sisoamp  $a4:5, $a0:1, $azeros, TAMP_F16V4_E4_P2
  }
  {
    st64pace      $a6:7, $tripacked_addr+=, $stride1, 0b00
    f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P3
  }
  {
    st64pace      $a4:5, $tripacked_addr+=, $stride1, 0b00
    f16v4sisoamp  $a4:5, $a0:1, $azeros, TAMP_F16V4_E4_P0
  }
  {
    st64pace      $a6:7, $tripacked_addr+=, $stride1, 0b10
    f16v4sisoamp  $a6:7, $a0:1, $azeros, TAMP_F16V4_E4_P1
  }

LNumElemsEq1:

  // This may need to change if partials for the next loop could be loaded
  // with the store of old results
  {
    st64pace      $a4:5,          $tripacked_addr+=, $stride1, 0b00
    f16v4sisoamp  $a4:5, $azeros, $azeros, TAMP_F16V4_E4_P2
  }
  {
    st64pace      $a6:7,          $tripacked_addr+=, $stride1, 0b00
    f16v4sisoamp  $a6:7, $azeros, $azeros, TAMP_F16V4_E4_P3
  }
  st64pace      $a4:5,          $tripacked_addr+=, $stride1, 0b00
  st64pace      $a6:7,          $tripacked_addr+=, $stride1, 0b00
Loop_outer_end:
  brnzdec $num_partitions, Loop_outer

L_end_fn:
exitz         $m15

.size convPartialFlattenedField, . - convPartialFlattenedField

// =============================================================================

.section ".text.\conv_partial_sup_contiguous\()", "ax"
.globl conv_partial_sup_contiguous 
.type conv_partial_sup_contiguous, @function

conv_partial_sup_contiguous:

// supervisor base is $m0 - passed to this function
#define sup_base                  m0
#define wkr_vertex                sp
#define stride_s                  m1
#define partition_s               m3
#define weights_vectors_s         m6
#define inchan_group_s            m3
#define inchan_vectors_s          m8
#define weight_vec_s              m7
#define outchan_vectors_s         m4
#define invec_s                   m2
#define conv_group_s              m5
#define outchan_group_s           m10
#define amp_group_s               m5
#define temp_s                    m9
#define outvec_s                  invec_s
#define wkr_function              m9
#define outstride_s               outchan_group_s

#define SUP_STACK_CALLEE_SAVE0    0  //(word)
#define SUP_STACK_CALLEE_SAVE1    4  //(word)
#define SUP_STACK_CALLEE_SAVE2    8  //(word)
#define SUP_STACK_CONV_GROUP      12 //(word)
// The number of output channels per group is divided by the number of number
// of channels the AMP processes. The actual value kept is one less.
#define SUP_STACK_AMP_GROUPS      16 //(word)
#define SUP_STACK_SIZE            24

// TODO: There are code sequences below which can be rearranged to reduce 
//       bubbles
// Performance:
// 80 + numConvGroups * (13 + 
//                       inChanGroups * (13 + 
//                                       outChanGroups * (11 + 
//                                                        AmpGroups * 22))))
// Where AMP groups = OutChansPerGroup / 8

// save $m4..6  and create space for vertex base
add           $sp, $sp, -(WKR_VERTEX_SIZE + SUP_STACK_SIZE)
ld32          $weights_vectors_s, $sup_base, SUP_WEIGHTS_VECTORS/4 
ld32          $stride_s, $sup_base, SUP_INPUT_STRIDE/4

// from base and delta, create base and expand pointer to vector list
// TODO: Optimise this to hide latencies if possible
ld32          $partition_s, $sup_base, SUP_PARTITION_BASEDELTA/4 
shl           $partition_s, $partition_s, 12
shr           $partition_s, $partition_s, 12
st32          $partition_s, $sp, WKR_PARTITION_BASE/4
ld32          $partition_s, $sup_base, (SUP_PARTITION_BASEDELTA + 4)/4
shl           $partition_s, $partition_s, 15
or            $partition_s, $partition_s, 0x80000000
shr           $partition_s, $partition_s, 13
st32          $partition_s, $wkr_vertex, WKR_PARTITION_LIST/4

st32          $m9, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CALLEE_SAVE1/4
st32          $m10, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CALLEE_SAVE2/4

ld32step      $weight_vec_s, $mzero, $weights_vectors_s+=, 1 
ld32          $inchan_vectors_s, $sup_base, SUP_INCHAN_VECTORS/4

ld32          $outchan_vectors_s, $sup_base, SUP_OUTCHAN_VECTORS/4
ld32          $inchan_group_s, $mzero, $sup_base, SUP_NUM_INCHAN_GROUPS_M1/4

ld32          $amp_group_s, $sup_base, SUP_OUTCHANS_PER_GROUP/4
shl           $amp_group_s, $amp_group_s, 2
st32          $amp_group_s,  $wkr_vertex, WKR_OUTCHANS_PER_GROUP/4
// additional shift because of multiply by 4 above
shr           $amp_group_s, $amp_group_s, (LOG2_AMP_OUTPUTS + 2)
add           $amp_group_s, $amp_group_s, -1
st32          $amp_group_s, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_AMP_GROUPS/4

put           $CCCSLOAD, $weight_vec_s

ld32          $conv_group_s, $sup_base, SUP_NUM_CONV_GROUPS_M1/4

ld32          $outstride_s, $sup_base, SUP_OUTSTRIDE / 4
shr           $outstride_s, $outstride_s, 1
and           $temp_s, $outstride_s, 0x3FF 
shl           $temp_s, $temp_s, 10
or            $stride_s, $stride_s, $temp_s
ld32step      $invec_s, $mzero, $inchan_vectors_s+=, 1    
st32          $stride_s, $wkr_vertex, WKR_IN_OUT_STRIDES/4
ld32          $outchan_group_s, $mzero, $sup_base, SUP_NUM_OUTCHAN_GROUPS_M1/4
setzi         $wkr_function, convPartialFlattenedField    

convGroupsLoop:
  st32          $conv_group_s, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CONV_GROUP/4
  // use a non-zero value to flag that zeroing of output must be done
  // this flag will be set to zero once first input channel group is processed
  st32          $weights_vectors_s, $wkr_vertex, WKR_ZERO_FLAG/4

inChanLoop:
    st32          $invec_s, $wkr_vertex,  WKR_INCHAN_PTR/4             
  
outChanLoop:
      ld32step     $weight_vec_s, $mzero, $weights_vectors_s+=, 1 
      ld32         $outvec_s, $mzero, $outchan_vectors_s, $outchan_group_s
      ld32         $amp_group_s, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_AMP_GROUPS/4

AmpGroupLoop:
        // must wait for all workers to load CWEI as it is shared
        sync          TEXCH_SYNCZONE_LOCAL
        ld128putcs    0
        ld128putcs    2
        ld128putcs    4
        ld128putcs    6
        ld128putcs    8
        ld128putcs    10
        ld128putcs    12
        ld128putcs    14
        ld128putcs    16
        ld128putcs    18
        st32          $outvec_s, $mzero, $wkr_vertex, WKR_OUTCHAN_PTR/4    
        ld128putcs    20
        ld128putcs    22
        ld128putcs    24
        ld128putcs    26
        ld128putcs    28
        ld128putcs    30
        add           $outvec_s, $outvec_s, AMP_OUTPUTS * SIZEOF_FLOAT
        runall        $wkr_function, $wkr_vertex, 0
        // increment output vector pointer for the AMP loop
        brnzdec $amp_group_s, AmpGroupLoop

      put           $CCCSLOAD, $weight_vec_s 
      brnzdec       $outchan_group_s, outChanLoop

    ld32          $outchan_group_s, $mzero, $sup_base, SUP_NUM_OUTCHAN_GROUPS_M1/4  
    // cannot write to worker vertex state until all workers have finished 
    // processing the output channel group 
    sync          TEXCH_SYNCZONE_LOCAL
    st32          $mzero, $wkr_vertex, WKR_ZERO_FLAG/4
    ld32step      $invec_s, $mzero, $inchan_vectors_s+=, 1    
    brnzdec       $inchan_group_s, inChanLoop

  ld32step      $mzero, $mzero, $outchan_vectors_s+=, $outchan_group_s
  ld32          $inchan_group_s, $mzero, $sup_base, SUP_NUM_INCHAN_GROUPS_M1/4
  ld32          $conv_group_s, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CONV_GROUP/4
  // TODO: add a delay instruction here
  add           $outchan_vectors_s, $outchan_vectors_s, 4
  brnzdec       $conv_group_s, convGroupsLoop

// restore stack
ld32          $m9, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CALLEE_SAVE1/4
ld32          $m10, $sp, WKR_VERTEX_SIZE/4 + SUP_STACK_CALLEE_SAVE2/4
add           $sp, $sp, WKR_VERTEX_SIZE + SUP_STACK_SIZE
br            $lr

.size conv_partial_sup_contiguous, . - conv_partial_sup_contiguous

#endif // #ifdef __IPU__
